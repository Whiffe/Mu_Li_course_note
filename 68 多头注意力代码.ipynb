{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84cbe0a-b33a-4b95-b65d-10fcc6cf1295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "091cc49f-cacf-4945-96da-9c7ee22aa097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "多头注意力（Multi-Head Attention）机制通过并行的多个注意力头（heads）\n",
    "来捕获不同子空间中的信息，每个头可以独立关注输入数据的不同方面，\n",
    "从而增强模型的表示能力。\n",
    "'''\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    '''\n",
    "    多头注意力\n",
    "    \n",
    "    - `key_size`、`query_size` 和 `value_size`：分别是键、查询和值的尺寸。\n",
    "    - `num_hiddens`：隐藏层的神经元数量。\n",
    "    - `num_heads`：注意力头的数量。\n",
    "    - `dropout`：Dropout层的概率。\n",
    "    - `W_q`、`W_k` 和 `W_v`：线性变换矩阵，用于将查询、键和值映射到多头注意力的输入空间。\n",
    "    - `W_o`：线性变换矩阵，用于将多个头的输出拼接在一起后再进行变换。\n",
    "\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 key_size, \n",
    "                 query_size, \n",
    "                 value_size, \n",
    "                 num_hiddens,\n",
    "                 num_heads, \n",
    "                 dropout,\n",
    "                 bias=False,\n",
    "                 **kwargs\n",
    "                ):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = d2l.DotProductAttention(dropout)\n",
    "        self.W_q = nn.Linear(query_size, num_hiddens, bias=bias)\n",
    "        self.W_k = nn.Linear(key_size, num_hiddens, bias=bias)\n",
    "        self.W_v = nn.Linear(value_size, num_hiddens, bias=bias)\n",
    "        self.W_o = nn.Linear(num_hiddens, num_hiddens, bias=bias)\n",
    "    def forward(self, queries, keys, values, valid_lens):\n",
    "        '''\n",
    "        - `queries`、`keys` 和 `values`：输入的查询、键和值。\n",
    "        - `valid_lens`：有效长度，用于掩蔽（mask）操作。\n",
    "        \n",
    "        1. 将查询、键和值通过线性变换，并使用 `transpose_qkv` 函数进行形状变换，\n",
    "            使其适应多头注意力的计算。\n",
    "\n",
    "        queries，keys，values的形状:\n",
    "            (batch_size，查询或者“键－值”对的个数，num_hiddens)\n",
    "        valid_lens　的形状:\n",
    "            (batch_size，)或(batch_size，查询的个数)\n",
    "            \n",
    "        经过变换后，输出的queries，keys，values　的形状:\n",
    "            (batch_size*num_heads，查询或者“键－值”对的个数，num_hiddens/num_heads)\n",
    "        \n",
    "        queries.shape: torch.Size([2, 4, 100])\n",
    "        keys.shape: torch.Size([2, 6, 100])\n",
    "        values.shape: torch.Size([2, 6, 100])\n",
    "        '''\n",
    "        queries = transpose_qkv(self.W_q(queries), self.num_heads)\n",
    "        keys = transpose_qkv(self.W_k(keys), self.num_heads)\n",
    "        values = transpose_qkv(self.W_v(values), self.num_heads)\n",
    "        '''\n",
    "        queries.shape: torch.Size([10, 4, 20])\n",
    "        keys.shape: torch.Size([10, 6, 20])\n",
    "        values.shape: torch.Size([10, 6, 20])\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        2. 如果有有效长度，则重复这些长度值，使其适应多头计算。\n",
    "        valid_lens: tensor([3, 2])\n",
    "        '''\n",
    "        if valid_lens is not None:\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0\n",
    "            )\n",
    "            \n",
    "        '''\n",
    "        valid_lens: tensor([3, 3, 3, 3, 3, 2, 2, 2, 2, 2])\n",
    "        \n",
    "        3. 通过注意力机制计算输出。\n",
    "        '''\n",
    "        output = self.attention(queries, keys, values, valid_lens)\n",
    "        \n",
    "        '''\n",
    "        output的形状:(batch_size*num_heads，查询的个数，num_hiddens/num_heads)\n",
    "        output.shape: torch.Size([10, 4, 20])\n",
    "        \n",
    "        4. 使用 `transpose_output` 函数逆转形状变换，将多个头的输出拼接在一起。\n",
    "        '''\n",
    "        output_concat = transpose_output(output, self.num_heads)\n",
    "        '''\n",
    "        output_concat的形状:(batch_size，查询的个数，num_hiddens)\n",
    "        output_concat.shape: torch.Size([2, 4, 100])\n",
    "        \n",
    "        5. 最后，通过线性变换得到最终的输出。\n",
    "        \n",
    "        self.W_o(output_concat).shape: torch.Size([2, 4, 100])\n",
    "        '''\n",
    "        return self.W_o(output_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "914f85c6-8d1a-44b0-9821-b55de9a2f016",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "这个函数用于将输入张量变换为适应多头注意力计算的形状。\n",
    "'''\n",
    "def transpose_qkv(X, num_heads):\n",
    "\n",
    "    '''\n",
    "    X.shape: torch.Size([2, 4, 100])\n",
    "    num_heads: 5\n",
    "    X.shape: torch.Size([2, 6, 100])\n",
    "    num_heads: 5\n",
    "    X.shape: torch.Size([2, 6, 100])\n",
    "    num_heads: 5\n",
    "    \n",
    "    - `X`：输入张量，形状为 `(batch_size, num_queries/num_kvpairs, num_hiddens)`。\n",
    "    - 将 `X` 变换为 `(batch_size, num_queries/num_kvpairs, num_heads, num_hiddens/num_heads)`。\n",
    "    '''\n",
    "    X = X.reshape(X.shape[0], X.shape[1], num_heads, -1)\n",
    "    \n",
    "    '''\n",
    "    X.shape: torch.Size([2, 4, 5, 20])\n",
    "    X.shape: torch.Size([2, 6, 5, 20])\n",
    "    X.shape: torch.Size([2, 6, 5, 20])\n",
    "    - 使用 `permute` 函数将张量的维度重排列为 `(batch_size, num_heads, num_queries/num_kvpairs, num_hiddens/num_heads)`。\n",
    "    '''\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    \n",
    "    '''\n",
    "    X.shape: torch.Size([2, 5, 4, 20])\n",
    "    X.shape: torch.Size([2, 5, 6, 20])\n",
    "    X.shape: torch.Size([2, 5, 6, 20])\n",
    "    - 最后，重新变换形状为 `(batch_size * num_heads, num_queries/num_kvpairs, num_hiddens/num_heads)`。\n",
    "    \n",
    "    X.reshape(-1, X.shape[2], X.shape[3]).shape: torch.Size([10, 4, 20])\n",
    "    X.reshape(-1, X.shape[2], X.shape[3]).shape: torch.Size([10, 6, 20])\n",
    "    X.reshape(-1, X.shape[2], X.shape[3]).shape: torch.Size([10, 6, 20])\n",
    "    '''\n",
    "    \n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cdbf155-9812-452a-a010-bbce2fc0dd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "这个函数用于逆转 `transpose_qkv` 函数的操作。\n",
    "\n",
    "- `X`：输入张量，形状为 `(batch_size * num_heads, num_queries/num_kvpairs, num_hiddens/num_heads)`。\n",
    "- 将 `X` 变换为 `(batch_size, num_heads, num_queries/num_kvpairs, num_hiddens/num_heads)`。\n",
    "- 使用 `permute` 函数将张量的维度重排列为 `(batch_size, num_queries/num_kvpairs, num_heads, num_hiddens/num_heads)`。\n",
    "- 最后，重新变换形状为 `(batch_size, num_queries/num_kvpairs, num_hiddens)`。\n",
    "'''\n",
    "def transpose_output(X, num_heads):\n",
    "    X = X.reshape(-1, num_heads, X.shape[1], X.shape[2])\n",
    "    X = X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(X.shape[0], X.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16c2453e-5ec3-4146-9165-219706e92f31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_k): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_v): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- `num_hiddens` 和 `num_heads`：分别表示隐藏层的神经元数量和注意力头的数量。\n",
    "'''\n",
    "\n",
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(\n",
    "    num_hiddens,\n",
    "    num_hiddens,\n",
    "    num_hiddens,\n",
    "    num_hiddens,\n",
    "    num_heads,\n",
    "    0.5\n",
    ")\n",
    "attention.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd13a163-7e78-49f4-9933-11970a50e835",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.W_o(output_concat).shape: torch.Size([2, 4, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- `batch_size` 和 `num_queries`：批量大小和查询数量。\n",
    "- `num_kvpairs` 和 `valid_lens`：键-值对的数量和有效长度。\n",
    "'''\n",
    "batch_size, num_queries = 2,4\n",
    "num_kvpairs, valid_lens = 6, torch.tensor([3, 2])\n",
    "'''\n",
    "- `X` 和 `Y`：输入的查询、键和值，都是形状为 `(batch_size, num_queries/num_kvpairs, num_hiddens)` 的张量。\n",
    "\n",
    "测试代码输出的形状为 `(batch_size, num_queries, num_hiddens)`，表示多头注意力机制的输出形状与输入查询的形状一致。\n",
    "'''\n",
    "X = torch.ones(\n",
    "    (batch_size, num_queries, num_hiddens)\n",
    ")\n",
    "Y = torch.ones(\n",
    "    (batch_size, num_kvpairs, num_hiddens)\n",
    ")\n",
    "attention(X, Y, Y, valid_lens).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb5dc3-f276-47a0-87e2-c769c675fc8a",
   "metadata": {},
   "source": [
    "多头注意力机制通过并行多个注意力头来增强模型的表示能力，能够捕获输入数据的不同方面。上述代码实现了多头注意力机制的核心步骤，包括查询、键和值的线性变换，形状变换，多头注意力计算，以及最终的线性变换输出。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
