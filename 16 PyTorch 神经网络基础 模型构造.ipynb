{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e92b79-81c7-4e11-90ac-de60adf4e51e",
   "metadata": {},
   "source": [
    "层和块\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf7798-5ce0-4480-8727-e3945fa76eb6",
   "metadata": {},
   "source": [
    "回顾多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "428d18cf-e667-49b1-b37b-fb86349fbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f44bb53e-1b12-469f-890c-ec94043950d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7af9172c-2e31-4872-b8f2-9f0d35e38fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "401c8777-6f6d-4e5d-8109-9384509a4748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0619, -0.1388, -0.0686,  0.2351, -0.0897,  0.2578,  0.0311,  0.1986,\n",
       "         -0.0372,  0.0620],\n",
       "        [-0.0264, -0.1562, -0.0033,  0.1439, -0.1902,  0.2907,  0.1291,  0.1637,\n",
       "         -0.1481,  0.1128]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9e896-e64c-4677-8b25-f17c3e2eeb20",
   "metadata": {},
   "source": [
    "自定义块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c47d66-ad2b-4872-aece-860696a8d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.out = nn.Linear(256,10)\n",
    "    def forward(self, X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff97b89c-4b74-41c4-b61b-867693b2ab17",
   "metadata": {
    "tags": []
   },
   "source": [
    "当然可以。下面是对你给出的代码的详细解释：\n",
    "\n",
    "首先，你定义了一个名为MLP的类，它继承自nn.Module。在PyTorch中，nn.Module是所有神经网络模块的基类，包括层、模型等。通过继承这个基类，你可以构建自己的神经网络模型。\n",
    "\n",
    "__init__ 方法\n",
    "__init__ 是一个特殊的方法，它在创建类的新实例时自动调用。在这个方法中，你初始化了网络的两层：\n",
    "\n",
    "self.hidden = nn.Linear(20, 256): 这一行定义了一个全连接层（或称为线性层），它接受一个大小为20的输入向量并输出一个大小为256的向量。这个层有 20 * 256 = 5120 个权重参数和 256 个偏置参数。\n",
    "\n",
    "self.out = nn.Linear(256, 10): 这一行定义了另一个全连接层，它接受一个大小为256的输入向量（来自self.hidden层的输出）并输出一个大小为10的向量。这个层有 256 * 10 = 2560 个权重参数和 10 个偏置参数。\n",
    "\n",
    "super().__init__() 是对父类 nn.Module 的初始化方法的调用，这是PyTorch要求的做法，以确保父类中的所有初始化操作都得到正确的执行。\n",
    "\n",
    "forward 方法\n",
    "forward 方法定义了数据通过网络的前向传播路径。当你将一个输入传递给一个神经网络模型时，这个方法会被调用。\n",
    "\n",
    "在这个例子中，forward 方法接受一个名为 X 的输入，该输入预期是一个大小为 (batch_size, 20) 的张量，其中 batch_size 是批次中的样本数量。\n",
    "\n",
    "self.hidden(X): 这一行将输入 X 传递给 self.hidden 层，该层会对输入进行线性变换。\n",
    "\n",
    "F.relu(...): 这是一个ReLU激活函数，它会对上一步的输出应用非线性变换。ReLU函数的定义是 f(x) = max(0, x)，它会将所有负值置为0，而正值则保持不变。\n",
    "\n",
    "self.out(...): 最后，经过ReLU激活函数处理后的输出被传递给 self.out 层进行第二次线性变换，得到最终的输出。\n",
    "\n",
    "这个 forward 方法最终返回的是一个大小为 (batch_size, 10) 的张量，其中每一行代表一个输入样本经过网络处理后的10个类别的预测得分。\n",
    "\n",
    "总结来说，这个 MLP 类定义了一个简单的多层感知机（MLP）模型，它有一个隐藏层和一个输出层。这个模型可以接受一个大小为20的输入向量，并输出一个大小为10的向量，通常用于10类分类问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f29eed-e1b8-4553-84f3-c4f3861541c8",
   "metadata": {},
   "source": [
    "实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c84dcd-fc7c-425c-860e-e4351e6dbc3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0292, -0.0904,  0.0961,  0.0554,  0.1272, -0.1335, -0.2312, -0.0484,\n",
       "         -0.2578,  0.0414],\n",
       "        [-0.0105, -0.0194,  0.1360, -0.0881, -0.0278, -0.1685, -0.3030, -0.0401,\n",
       "         -0.1090,  0.0154]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b5a5bb-a0c2-43ba-90b0-b467ace84558",
   "metadata": {},
   "source": [
    "顺序块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f4d24ef-ebcb-42a3-89dd-1e656d2b854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block] = block\n",
    "    \n",
    "    def forward(self, X):\n",
    "        for block in self._modules.values():\n",
    "            X = block(X)\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648cafc2-38c1-4fb7-83ba-d79465dee7f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1263, -0.2223, -0.0129,  0.1760,  0.0761,  0.1645,  0.1349, -0.2172,\n",
       "         -0.0012,  0.0541],\n",
       "        [ 0.1140, -0.3457, -0.0431,  0.2171,  0.0350,  0.1335,  0.1719, -0.0383,\n",
       "         -0.0190, -0.0449]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256,10))\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5ff990-3990-47cd-ae90-6c597f2199b3",
   "metadata": {},
   "source": [
    "这段代码定义了一个自定义的序贯模型MySequential，它模仿了PyTorch内置的nn.Sequential的行为。下面是对代码的详细解释：\n",
    "\n",
    "定义MySequential类\n",
    "MySequential类继承自nn.Module，这意味着它是一个可以被用在神经网络中的组件。\n",
    "\n",
    "__init__方法\n",
    "在__init__方法中，*args允许传入任意数量的参数，这些参数预期是神经网络层或其他模块。对于传入的每一个block（层或模块），它都被添加到self._modules字典中。self._modules是nn.Module的一个特殊属性，用于存储子模块。\n",
    "\n",
    "python\n",
    "def __init__(self, *args):  \n",
    "    super().__init__()  \n",
    "    for block in args:  \n",
    "        self._modules[block] = block\n",
    "这里有一点需要注意的是，通常我们会在self._modules中使用唯一的字符串键来存储子模块，但在这个例子中直接使用了模块对象作为键。虽然这在技术上是可行的，但通常不推荐这样做，因为它可能会导致混淆或不可预见的行为。\n",
    "\n",
    "forward方法\n",
    "forward方法定义了数据通过模型的前向传播路径。它遍历self._modules中的所有值（即传入的层或模块），并将输入X依次传递给这些层或模块。\n",
    "\n",
    "python\n",
    "def forward(self, X):  \n",
    "    for block in self._modules.values():  \n",
    "        X = block(X)  \n",
    "    return X\n",
    "这里，X = block(X)表示将X作为输入传递给当前的block（层或模块），并将输出重新赋值给X。这样，在遍历完所有层或模块后，X将包含最终的输出。\n",
    "\n",
    "使用MySequential类\n",
    "接下来，代码创建了一个MySequential的实例net，并传入了三个层：一个线性层、一个ReLU激活函数层和一个输出层。\n",
    "\n",
    "python\n",
    "net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "这相当于创建了一个序贯模型，其结构如下：\n",
    "\n",
    "输入层 -> 线性层 (20 -> 256) -> ReLU激活函数 -> 线性层 (256 -> 10) -> 输出层\n",
    "最后，通过调用net(X)，输入数据X将通过这个序贯模型进行前向传播，并返回最终的输出。\n",
    "\n",
    "注意点\n",
    "模块键的问题：如前所述，直接在self._modules中使用模块对象作为键可能不是一个好主意。更常见的做法是使用字符串键，如self._modules['linear'] = nn.Linear(20, 256)。\n",
    "\n",
    "输入数据X的维度：在这段代码中，没有提及输入数据X的具体维度。但从上下文来看，我们可以假设X的形状是(batch_size, 20)，其中batch_size是批次中的样本数量。\n",
    "\n",
    "缺少导入语句：为了使这段代码正常工作，你需要确保已经导入了必要的库和模块，例如：\n",
    "\n",
    "python\n",
    "import torch  \n",
    "import torch.nn as nn\n",
    "总的来说，这段代码定义了一个简单的自定义序贯模型，并展示了如何使用它来进行前向传播。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c352b20d-c8f8-48b9-810a-45b1362f76a6",
   "metadata": {},
   "source": [
    "在正向传播函数中执行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a908ca3c-fc0a-44e4-85cf-168abffcc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight = torch.rand((20, 20), requires_grad=False)\n",
    "        self.linear = nn.Linear(20, 20)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        X = self.linear(X)\n",
    "        X = F.relu(torch.mm(X, self.rand_weight) + 1)\n",
    "        X = self.linear(X)\n",
    "        while X.abs().sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55cec8e4-d249-4645-9554-65b431d1cb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0393, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfba761-9f44-4311-8a02-4536023749bf",
   "metadata": {},
   "source": [
    "这段代码定义了一个名为FixedHiddenMLP的神经网络模型，该模型继承自nn.Module，即PyTorch中的基础神经网络模块。下面我将详细解释这段代码的每个部分：\n",
    "\n",
    "1. __init__ 方法\n",
    "在__init__方法中，模型进行了初始化。\n",
    "\n",
    "super().__init__(): 调用父类nn.Module的初始化方法。\n",
    "self.rand_weight = torch.rand((20, 20), requires_grad=False): 创建一个20x20的随机权重矩阵，并且设置requires_grad=False，这意味着在训练过程中不会更新这个权重矩阵。\n",
    "self.linear = nn.Linear(20, 20): 创建一个线性层，输入和输出特征数都是20。\n",
    "注意：在self.rand_weight的初始化中，括号不匹配，应该是self.rand_weight = torch.rand((20, 20), requires_grad=False)。\n",
    "\n",
    "2. forward 方法\n",
    "forward方法定义了数据通过模型的前向传播路径。\n",
    "\n",
    "X = self.linear(X): 输入数据X首先通过一个线性层。\n",
    "X = F.relu(torch.mm(X, self.rand_weight) + 1): 然后，X与固定的随机权重矩阵self.rand_weight进行矩阵乘法，并加上1。接着，通过ReLU激活函数。\n",
    "X = self.linear(X): 再次通过线性层。\n",
    "while X.abs().sum() > 1: X /= 2: 如果X的L1范数（即所有元素绝对值的和）大于1，则将X的每个元素除以2，直到L1范数小于或等于1。这是一个归一化步骤，确保X的L1范数在1以内。\n",
    "return X.sum(): 最后，返回X中所有元素的和。\n",
    "3. 创建模型实例和调用\n",
    "net = FixedHiddenMLP(): 创建一个FixedHiddenMLP的实例。\n",
    "net(X): 用X作为输入调用模型。这里X应该是一个形状为(batch_size, 20)的张量，其中batch_size是批次中的样本数量。\n",
    "总结\n",
    "这个FixedHiddenMLP模型是一个相对简单的神经网络，它包含两个线性层和一个固定的随机权重矩阵。模型输出是输入数据经过一系列变换后的所有元素的和，且这些变换包括线性变换、ReLU激活和L1范数归一化。由于其中一个权重矩阵是固定的，所以在训练过程中不会更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf277bb3-9c4d-4037-b28d-f55962f71cde",
   "metadata": {},
   "source": [
    "FixedHiddenMLP这个模型的设计具有一定的特殊性和目的。固定随机权重矩阵的使用可以带来几个潜在的好处或考虑因素：\n",
    "\n",
    "模型简化：通过使用固定的随机权重矩阵，我们消除了学习该特定权重矩阵的需要。这可以简化模型，减少训练时需要的计算资源和时间。\n",
    "\n",
    "特征变换：尽管权重是随机的且固定的，但它们仍然可以对输入数据进行有意义的变换。这种变换可能有助于模型捕捉数据中的某些模式或结构，尤其是当这些模式不容易通过常规线性或非线性层捕捉时。\n",
    "\n",
    "正则化：通过引入固定的随机性，模型可能具有某种形式的正则化效果。正则化是防止模型过拟合训练数据的一种技术。通过使用固定的随机权重，模型在训练过程中不会过度依赖于这些权重，这有助于保持模型的泛化能力。\n",
    "\n",
    "模型组合：固定随机权重可以被视为一种特征工程技术，与其他可学习的层结合使用可以产生丰富的特征表示。这种组合可能使模型能够学习更复杂的函数或模式。\n",
    "\n",
    "随机性引入：在某些情况下，引入随机性可以提高模型的性能。固定的随机权重可以看作是一种在模型架构中显式地引入随机性的方式。\n",
    "\n",
    "计算效率：由于权重是固定的，因此在前向传播过程中不需要进行权重更新，这可以提高计算效率，特别是在资源有限或需要高效推断的场景下。\n",
    "\n",
    "然而，固定随机权重矩阵的使用也有其局限性。由于权重是固定的，模型可能无法适应数据中的某些变化或模式，这可能会限制模型的性能。此外，固定的随机性可能不适用于所有任务或数据集，因此需要根据具体情况来决定是否使用这种技术。\n",
    "\n",
    "总之，FixedHiddenMLP的设计是出于对计算效率、模型正则化和特定特征变换的考虑。它展示了在神经网络设计中结合可学习和固定组件的灵活性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea3075-905f-435b-b764-004bd6172251",
   "metadata": {},
   "source": [
    "混合搭配各种组合块的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c7522f-1d24-449c-8107-b9ba8a8807ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(20, 64),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64, 32),\n",
    "                                 nn.ReLU()\n",
    "                                )\n",
    "        self.linear = nn.Linear(32, 16)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.linear(self.net(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "499e83fc-6e62-4a07-9344-6ea3edf9a7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2197, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chimera = nn.Sequential(\n",
    "                        NestMLP(),\n",
    "                        nn.Linear(16, 20),\n",
    "                        FixedHiddenMLP()\n",
    "                       )\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d8950-1903-4c33-bc84-c464e9f43f75",
   "metadata": {},
   "source": [
    "好的，我将为您详细解释这段代码。\n",
    "\n",
    "首先，让我们从NestMLP类开始：\n",
    "\n",
    "NestMLP类\n",
    "NestMLP是一个神经网络模型，继承自nn.Module。它包含两个主要的组件：\n",
    "\n",
    "self.net：这是一个nn.Sequential模型，它包含两个线性层（nn.Linear）和两个ReLU激活函数。数据首先通过第一个线性层，其输入特征数为20，输出特征数为64。然后，数据经过一个ReLU激活函数。接着，数据进入第二个线性层，其输入特征数为64，输出特征数为32。最后，数据再次经过一个ReLU激活函数。\n",
    "\n",
    "python\n",
    "self.net = nn.Sequential(  \n",
    "    nn.Linear(20, 64),  \n",
    "    nn.ReLU(),  \n",
    "    nn.Linear(64, 32),  \n",
    "    nn.ReLU()  \n",
    ")\n",
    "self.linear：这是一个线性层，其输入特征数为32（与self.net的输出特征数相匹配），输出特征数为16。\n",
    "\n",
    "python\n",
    "self.linear = nn.Linear(32, 16)\n",
    "在forward方法中，数据首先通过self.net，然后通过self.linear。这两个操作是顺序进行的，即self.net的输出作为self.linear的输入。\n",
    "\n",
    "python\n",
    "def forward(self, X):  \n",
    "    return self.linear(self.net(X))\n",
    "Chimera模型\n",
    "Chimera是一个nn.Sequential模型，它组合了三个组件：\n",
    "\n",
    "NestMLP()：我们之前定义的NestMLP模型。\n",
    "nn.Linear(16, 20)：一个线性层，其输入特征数为16（与NestMLP的输出特征数相匹配），输出特征数为20。\n",
    "FixedHiddenMLP()：这是您之前提供的另一个模型，其中包含一个固定的随机权重矩阵和两个线性层。\n",
    "当您使用chimera(X)调用此模型时，输入数据X首先通过NestMLP，然后通过线性层，最后通过FixedHiddenMLP。这三个操作也是顺序进行的，即每个组件的输出作为下一个组件的输入。\n",
    "\n",
    "简而言之，Chimera模型是一个更复杂的神经网络，由NestMLP、一个线性层和FixedHiddenMLP组成，它们按顺序连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba3cb1-fc58-4852-a096-c4eb16bbca87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
