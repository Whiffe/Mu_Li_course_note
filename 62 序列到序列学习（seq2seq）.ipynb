{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb458cb4-d9a4-4a1b-bf84-d795f653cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d2513-e4e2-400c-8e33-34b7557cc935",
   "metadata": {},
   "source": [
    "编码器\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f50579-df58-4fc6-a536-f42d0094ef26",
   "metadata": {},
   "source": [
    "嵌入层（Embedding Layer）：用于将输入的词元（单词、字符等）转换为特征向量，这些特征向量捕捉了词元的语义信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d4f86-8721-496b-b78a-a6f41874a300",
   "metadata": {},
   "source": [
    "门控循环单元（GRU）：一种改进的 RNN 单元，解决了传统 RNN 中的梯度消失问题，更适合处理长序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c89d2bac-f330-4c96-86c6-2c3baf2dc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vocab_size：词表大小，即可以处理的不同词元的数量。\n",
    "embed_size：每个词元的特征向量的维度。\n",
    "num_hiddens：隐藏层的神经元数量，即隐状态的维度。\n",
    "num_layers：GRU 的层数。\n",
    "\n",
    "\n",
    "`super(Seq2SeqEncoder, self).__init__(**kwargs)` \n",
    "这一行代码的作用和具体意思可以通过以下几点来解释：\n",
    "\n",
    "1. 继承\n",
    "`Seq2SeqEncoder` 继承自 `d2l.Encoder`，所以它是 `d2l.Encoder` 的一个子类。\n",
    "继承意味着 `Seq2SeqEncoder` 会继承 `d2l.Encoder` 类中的所有方法和属性。\n",
    "\n",
    "2. 调用父类的构造函数\n",
    "`super(Seq2SeqEncoder, self)` 是 Python 中调用父类（即基类）的方法。\n",
    "这里的 `super` 函数返回当前类 `Seq2SeqEncoder` 的父类 `d2l.Encoder`。\n",
    "\n",
    "3. 初始化父类\n",
    "`super(Seq2SeqEncoder, self).__init__(**kwargs)` 调用了父类 `d2l.Encoder`\n",
    "的构造函数 `__init__` 方法，并将关键字参数 `**kwargs` 传递给它。\n",
    "这样可以确保 `d2l.Encoder` 的初始化逻辑被执行。\n",
    "\n",
    "4. 参数传递\n",
    "`**kwargs` 表示任意数量的关键字参数。这意味着你可以向 `Seq2SeqEncoder` \n",
    "传递一些参数，这些参数会被传递给 `d2l.Encoder` 的构造函数。\n",
    "\n",
    "在初始化方法中：\n",
    "\n",
    "self.embedding 是一个嵌入层，它把词元的索引映射到特征向量。\n",
    "self.rnn 是一个多层 GRU，用于处理序列数据。\n",
    "\n",
    "'''\n",
    "\n",
    "class Seq2SeqEncoder(d2l.Encoder):\n",
    "    #用于序列到序列学习的循环神经网络编码器\n",
    "    def __init__(\n",
    "    self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs\n",
    "    ):\n",
    "        super(Seq2SeqEncoder, self).__init__(**kwargs)\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_size, num_hiddens, num_layers, dropout=dropout\n",
    "        )\n",
    "    '''\n",
    "    X 是输入，形状为 (batch_size, num_steps)，其中 batch_size 是批量大小，\n",
    "    num_steps 是序列长度。\n",
    "    \n",
    "    self.embedding(X) 将输入的词元索引转换为特征向量，\n",
    "    形状变为 (batch_size, num_steps, embed_size)。\n",
    "    \n",
    "    X.permute(1, 0, 2) 将维度顺序调整为\n",
    "    (num_steps, batch_size, embed_size)，因为 GRU 需要第一个维度是时间步。\n",
    "    \n",
    "    self.rnn(X) 运行 GRU，得到 output 和 state：\n",
    "        output 是每个时间步的输出，形状为 \n",
    "        (num_steps, batch_size, num_hiddens)。\n",
    "        \n",
    "        state 是每层最后一个时间步的隐状态，形状为 \n",
    "        (num_layers, batch_size, num_hiddens)。\n",
    "        \n",
    "    '''\n",
    "    def forward(self, X, *args):\n",
    "        # 输入‘X’的形状：（batch_size, num_steps, embed_size）\n",
    "        X = self.embedding(X)\n",
    "        # 在循环神经网络模型中，第一个轴对应时间步\n",
    "        X = X.permute(1, 0, 2)\n",
    "        # 如果未提及状态，则默认为0\n",
    "        output, state = self.rnn(X)\n",
    "        # output的形状：(num_steps, batch_size, num_hiddens)\n",
    "        # state的形状：(num_layers, batch_size, num_hiddens)\n",
    "        return output, state\n",
    "    \n",
    "    '''\n",
    "    总结\n",
    "        嵌入层：将输入序列中的词元索引转换为特征向量。\n",
    "        GRU：处理特征向量序列，生成每个时间步的输出和最终的隐状态。\n",
    "        前向传播：将输入通过嵌入层，再通过 GRU，得到输出和隐状态。\n",
    "        这个编码器的设计允许它处理可变长度的输入序列，并将序列信息编码到一个固定形状的隐状态中，从而为解码器生成输出序列提供信息。\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee22beee-bee5-455a-829a-7f13a05858f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqEncoder(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (rnn): GRU(8, 16, num_layers=2)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "vocab_size=10：词汇表大小是10，表示可以有10种不同的词。\n",
    "embed_size=8：每个词被嵌入到一个8维的向量中。\n",
    "num_hiddens=16：每个隐藏层有16个隐藏单元。\n",
    "num_layers=2：编码器由2层GRU组成。\n",
    "\n",
    "实例化编码器\n",
    "'''\n",
    "encoder = Seq2SeqEncoder(\n",
    "    vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2\n",
    ")\n",
    "# 设置编码器为评估模式\n",
    "encoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "155fb3fb-9170-4b25-953f-c521c3b8f2e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 4, 16])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义输入序列\n",
    "X = torch.zeros((4,7), dtype=torch.long)\n",
    "# 通过编码器处理输入\n",
    "output, state = encoder(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc2e20ba-c722-49f8-81c3-15795a93c5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.shape: torch.Size([7, 4, 16])\n",
      "state.shape: torch.Size([2, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "output.shape: torch.Size([7, 4, 16])\n",
    "\n",
    "时间步数（7）：这个维度表示序列的时间步数。这里是7，表示每个输入序列有7个时间步。\n",
    "\n",
    "批量大小（4）：这个维度表示批量大小。这里是4，表示我们一次处理4个序列。\n",
    "\n",
    "隐藏单元数（16）：这个维度表示每个时间步的隐藏单元数。\n",
    "这里是16，表示GRU的隐藏状态有16个单元。\n",
    "\n",
    "7个时间步、4个序列、每个序列在每个时间步都有16维的隐藏状态，所以形状是 [7, 4, 16]。\n",
    "\n",
    "'''\n",
    "print(\"output.shape:\",output.shape)\n",
    "\n",
    "'''\n",
    "state.shape: torch.Size([2, 4, 16])\n",
    "\n",
    "隐藏层数量（2）：这个维度表示隐藏层的数量。这里是2，表示编码器有2层GRU。\n",
    "\n",
    "批量大小（4）：这个维度表示批量大小。这里是4，表示我们一次处理4个序列。\n",
    "\n",
    "隐藏单元数（16）：这个维度表示每个隐藏层的隐藏单元数。\n",
    "这里是16，表示每个GRU层的隐藏状态有16个单元。\n",
    "\n",
    "2层GRU、4个序列、每层的隐藏状态有16维，所以形状是 [2, 4, 16]。\n",
    "\n",
    "'''\n",
    "print(\"state.shape:\",state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f317cf76-4c75-4393-be4a-edfcefbca96c",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c14c76-0ee7-41b3-bcad-2e552946fd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "解码器的工作原理\n",
    "\n",
    "在序列到序列模型中，解码器的输入是编码器的输出（隐状态）和上一步解码器的输出。\n",
    "为了确保解码器能访问整个输入序列的编码信息，\n",
    "我们将编码器的最后一层的隐状态重复并拼接到解码器的每一个时间步的输入中。\n",
    "\n",
    "在序列到序列（Seq2Seq）模型中，解码器的任务是根据编码器生成的隐状态和上下文向量逐步生成输出序列。\n",
    "每个时间步的输出不仅依赖于当前的输入，还依赖于之前的输出和隐状态。\n",
    "\n",
    "\n",
    "\n",
    "vocab_size：词汇表大小。\n",
    "\n",
    "embed_size：嵌入层的维度。\n",
    "\n",
    "num_hiddens：GRU隐藏单元的数量。\n",
    "\n",
    "num_layers：GRU层数。\n",
    "\n",
    "dropout：dropout概率。\n",
    "\n",
    "self.embedding：将词元索引映射到特征向量的嵌入层。\n",
    "\n",
    "self.rnn：多层GRU，输入是特征向量和隐状态。\n",
    "\n",
    "self.dense：全连接层，将GRU的输出转换为词元的概率分布。\n",
    "\n",
    "'''\n",
    "\n",
    "class Seq2SeqDecoder(d2l.Decoder):\n",
    "    # 用于序列到序列学习的循环神经网络解码器\n",
    "    def __init__(\n",
    "        self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs\n",
    "    ):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_size + num_hiddens,\n",
    "            num_hiddens,\n",
    "            num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "    '''\n",
    "    初始化解码器状态 init_state\n",
    "    \n",
    "    enc_outputs：编码器的输出。\n",
    "    返回编码器的隐状态，作为解码器的初始隐状态。\n",
    "    '''\n",
    "    def init_state(self, enc_outputs, *args):\n",
    "        return enc_outputs[1]\n",
    "    '''\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    def forward(self, X, state):\n",
    "        # 输出‘X’的形状：（batch_size, num_steps, embed_size）\n",
    "        '''\n",
    "        1. 嵌入层转换\n",
    "            - `X`：输入序列。\n",
    "            - `self.embedding(X)`：将词元索引转换为特征向量，\n",
    "                形状为 `(batch_size, num_steps, embed_size)`。\n",
    "            - `permute(1, 0, 2)`：调整维度顺序为 `(num_steps, batch_size, embed_size)`。\n",
    "            \n",
    "        X.shape: torch.Size([4, 7])\n",
    "        self.embedding(X).shape: torch.Size([4, 7, 8])\n",
    "        self.embedding(X).permute(1, 0, 2).shape: torch.Size([7, 4, 8])\n",
    "        '''\n",
    "        X = self.embedding(X).permute(1, 0, 2)\n",
    "        \n",
    "        '''\n",
    "        2. 上下文向量\n",
    "            - `state[-1].repeat(X.shape[0], 1, 1)`：将上下文向量复制以匹配输入序列的时间步数，\n",
    "            形状为 `(num_steps, batch_size, num_hiddens)`。\n",
    "\n",
    "            `state[-1]`\n",
    "                - `state` 是编码器的隐状态，形状为 `[num_layers, batch_size, num_hiddens]`。\n",
    "                - `state[-1]` 选择的是编码器的最后一层的隐状态，形状为 `[batch_size, num_hiddens]`。\n",
    "            假设 `state` 的形状是 `[2, 4, 16]`，那么 `state[-1]` 的形状是 `[4, 16]`。\n",
    "        \n",
    "\n",
    "        `repeat(X.shape[0], 1, 1)`\n",
    "            - `repeat` 用于沿指定的维度重复张量。\n",
    "            - `state[-1].repeat(X.shape[0], 1, 1)` 表示将 `state[-1]` 沿第一个维度重复 `X.shape[0]` 次。\n",
    "        假设 `state[-1]` 的形状是 `[4, 16]`，且 `X.shape[0]` 是 `7`，那么 `state[-1].repeat(7, 1, 1)` 的形状是 `[7, 4, 16]`。\n",
    "\n",
    "        \n",
    "        X.shape: torch.Size([7, 4, 8])\n",
    "        \n",
    "        state.shape: torch.Size([2, 4, 16])\n",
    "        state[-1].shape: torch.Size([4, 16])\n",
    "        \n",
    "        context.shape: torch.Size([7, 4, 16])\n",
    "        '''\n",
    "        # 广播context，使其具有与X相同的num_steps\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1)\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        3. 拼接输入和上下文：\n",
    "            - `torch.cat((X, context), 2)`：在最后一个维度拼接特征向量和上下文向量，\n",
    "            形状为 `(num_steps, batch_size, embed_size + num_hiddens)`。\n",
    "        \n",
    "        X_and_context.shape: torch.Size([7, 4, 24])\n",
    "        '''\n",
    "        X_and_context = torch.cat((X, context), 2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        '''\n",
    "        4. 通过GRU\n",
    "            - `self.rnn(X_and_context, state)`：计算GRU的输出和新隐状态。\n",
    "            - `output`：形状为 `(num_steps, batch_size, num_hiddens)`。\n",
    "            - `state`：形状为 `(num_layers, batch_size, num_hiddens)`。\n",
    "\n",
    "        output.shape torch.Size([7, 4, 16])\n",
    "        state.shape torch.Size([2, 4, 16])\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        \n",
    "        '''\n",
    "        5. 输出层\n",
    "            - `self.dense(output)`：将GRU的输出转换为词元的概率分布。\n",
    "            - `permute(1, 0, 2)`：调整维度顺序为 `(batch_size, num_steps, vocab_size)`。\n",
    "\n",
    "        output.shape torch.Size([4, 7, 10])\n",
    "        '''\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        \n",
    "        # output的形状：(batch_size, num_steps, vocab_size)\n",
    "        # state的形状：(num_layers, batch_size, num_hiddens)\n",
    "        return output, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e9c2fc-82ba-4fec-897e-6f64291b4d3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqDecoder(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (rnn): GRU(24, 16, num_layers=2)\n",
       "  (dense): Linear(in_features=16, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Seq2SeqDecoder(\n",
    "    vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2\n",
    ")\n",
    "\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3c11c1-9d69-4c1d-8f2b-2a71f4950f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), torch.Size([2, 4, 16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = decoder.init_state(encoder(X))\n",
    "output, state = decoder(X, state)\n",
    "output.shape, state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a96068-517a-4c85-89d9-19a8ee2745bb",
   "metadata": {},
   "source": [
    "损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "973a6023-3d51-4569-adec-87848b30f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "在序列到序列（seq2seq）模型中，我们经常需要对不同长度的序列进行批量处理。\n",
    "为了使这些序列具有相同的形状，我们会在短序列的末尾填充特殊的填充词元。\n",
    "训练过程中，我们不希望填充词元影响损失的计算，因此需要屏蔽这些不相关的填充词元。\n",
    "sequence_mask 函数就是用来实现这个目的的。\n",
    "\n",
    "\n",
    "参数解释\n",
    "\n",
    "- `X`：输入张量，形状为 `[batch_size, maxlen]`，其中 `maxlen` 是序列的最大长度。\n",
    "- `valid_len`：每个序列的有效长度，即不包括填充词元的实际长度。形状为 `[batch_size]`。\n",
    "- `value`：要用来屏蔽不相关项的值，默认为 `0`。\n",
    "\n",
    "'''\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    # 在序列中屏蔽不相关的项\n",
    "    '''\n",
    "    X.shape: torch.Size([2, 3])\n",
    "    valid_len.shape: torch.Size([2])\n",
    "    \n",
    "\n",
    "    1. 获取最大长度\n",
    "\n",
    "       ```python\n",
    "       maxlen = X.size(1)\n",
    "       ```\n",
    "\n",
    "       获取输入张量 `X` 的第二维度（时间步长度）的大小，即最大长度 `maxlen`。\n",
    "    '''\n",
    "    maxlen = X.size(1)\n",
    "    '''\n",
    "    maxlen: 3\n",
    "\n",
    "    '''\n",
    "    mask = torch.arange(\n",
    "        (maxlen),\n",
    "        dtype=torch.float32,\n",
    "        device=X.device\n",
    "    )[None, :] < valid_len[:, None]\n",
    "    \n",
    "    '''\n",
    "    mask: tensor([[ True, False, False],\n",
    "        [ True,  True, False]])\n",
    "    X: tensor([[1, 2, 3],\n",
    "            [4, 5, 6]])\n",
    "    \n",
    "    torch.arange((maxlen),dtype=torch.float32,device=X.device)[None, :].shape: torch.Size([1, 3])\n",
    "    torch.arange((maxlen),dtype=torch.float32,device=X.device)[None, :]: tensor([[0., 1., 2.]])\n",
    "    \n",
    "    valid_len[:, None].shape: torch.Size([2, 1])\n",
    "    valid_len[:, None]: tensor([[1],\n",
    "        [2]])\n",
    "    \n",
    "    mask.shape: torch.Size([2, 3])\n",
    "    \n",
    "    2. **创建掩码**\n",
    "\n",
    "       ```python\n",
    "       mask = torch.arange((maxlen), dtype=torch.float32,\n",
    "                           device=X.device)[None, :] < valid_len[:, None]\n",
    "       ```\n",
    "\n",
    "       - `torch.arange((maxlen), dtype=torch.float32, device=X.device)` 创建一个从 `0` 到 `maxlen-1` 的一维张量。\n",
    "       - `[None, :]` 增加一个维度，使其形状变为 `[1, maxlen]`。\n",
    "       - `valid_len[:, None]` 将 `valid_len` 的形状从 `[batch_size]` 变为 `[batch_size, 1]`。\n",
    "       - 比较操作 `<` 会广播，使得 `mask` 的形状变为 `[batch_size, maxlen]`，其中每个元素表示该位置是否在有效长度内。\n",
    "    '''\n",
    "    X[~mask] = value\n",
    "    '''\n",
    "    X: tensor([[1, 0, 0],\n",
    "            [4, 5, 0]])\n",
    "            \n",
    "    3. **应用掩码**\n",
    "\n",
    "       ```python\n",
    "       X[~mask] = value\n",
    "       ```\n",
    "\n",
    "       - `~mask` 是 `mask` 的按位取反，表示填充部分。\n",
    "       - `X[~mask] = value` 将 `X` 中不在有效长度内的部分设置为 `value`（默认是 `0`）。\n",
    "\n",
    "    '''\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3078f4d8-04fe-4e6a-8576-743c5d44eebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0],\n",
       "        [4, 5, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(\n",
    "    [\n",
    "        [1, 2, 3],\n",
    "        [4, 5, 6]\n",
    "    ]\n",
    ")\n",
    "sequence_mask(X, torch.tensor([1, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dc3ee73-a307-4090-b402-5b51079c8211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.],\n",
       "         [-1., -1., -1., -1.]],\n",
       "\n",
       "        [[ 1.,  1.,  1.,  1.],\n",
       "         [ 1.,  1.,  1.,  1.],\n",
       "         [-1., -1., -1., -1.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones(2, 3, 4)\n",
    "sequence_mask(X, torch.tensor([1, 2]), value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26308f43-21eb-4c6e-b23a-41874899f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "自定义的带遮蔽的Softmax交叉熵损失函数 `MaskedSoftmaxCELoss` 以及示例代码。\n",
    "\n",
    "`MaskedSoftmaxCELoss` 类\n",
    "\n",
    "这个类继承了 `torch.nn.CrossEntropyLoss`，并重写了 `forward` 方法以实现带遮蔽的损失计算。\n",
    "\n",
    "'''\n",
    "\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    # 带遮蔽的softmax交叉熵损失函数\n",
    "    # pred的形状：（batch_size, num_steps, vocab_size）\n",
    "    # label的形状：(batch_size, num_steps)\n",
    "    # valid_len的形状：(batch_size,)\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        '''\n",
    "        1. 创建权重张量\n",
    "\n",
    "           ```python\n",
    "           weights = torch.ones_like(label)\n",
    "           ```\n",
    "\n",
    "           创建一个与 `label` 形状相同的全1张量，作为初始的权重张量。\n",
    "        \n",
    "         weights: tensor([[1, 1, 1, 1],\n",
    "                [1, 1, 1, 1],\n",
    "                [1, 1, 1, 1]])\n",
    "        '''\n",
    "        weights = torch.ones_like(label)\n",
    "        '''\n",
    "        2. 应用序列掩码\n",
    "\n",
    "           ```python\n",
    "           weights = sequence_mask(weights, valid_len)\n",
    "           ```\n",
    "\n",
    "           使用 `sequence_mask` 函数将权重张量中无效长度（填充部分）的位置设置为0。\n",
    "           \n",
    "        weights: tensor([[1, 1, 1, 1],\n",
    "            [1, 1, 0, 0],\n",
    "            [0, 0, 0, 0]])\n",
    "\n",
    "        '''\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        '''\n",
    "        3. 设置不进行自动归约\n",
    "\n",
    "           ```python\n",
    "           self.reduction = 'none'\n",
    "           ```\n",
    "\n",
    "           将损失函数的归约方式设置为 `none`，以便获取逐元素的损失值，而不是直接求和或平均。\n",
    "        '''\n",
    "        self.reduction='none'\n",
    "        '''\n",
    "        \n",
    "        4. 计算未加权的交叉熵损失\n",
    "\n",
    "           ```python\n",
    "           unweighted_loss = super(MaskedSoftmaxCELoss, self).forward(\n",
    "               pred.permute(0, 2, 1), label)\n",
    "           ```\n",
    "           \n",
    "           super(MaskedSoftmaxCELoss, self).forward(pred.permute(0, 2, 1), label) 的作用是\n",
    "           调用 nn.CrossEntropyLoss 类的 forward 方法。\n",
    "\n",
    "           - `pred.permute(0, 2, 1)`：将预测张量的形状从 \n",
    "           `(batch_size, num_steps, vocab_size)` 变为 \n",
    "           `(batch_size, vocab_size, num_steps)`，\n",
    "           以匹配 `nn.CrossEntropyLoss` 的输入要求。\n",
    "           - 计算未加权的交叉熵损失。\n",
    "           \n",
    "        unweighted_loss: tensor([[2.3026, 2.3026, 2.3026, 2.3026],\n",
    "                [2.3026, 2.3026, 2.3026, 2.3026],\n",
    "                [2.3026, 2.3026, 2.3026, 2.3026]])\n",
    "\n",
    "        '''\n",
    "        unweighted_loss = super(\n",
    "            MaskedSoftmaxCELoss, self\n",
    "        ).forward(\n",
    "            pred.permute(0, 2, 1),\n",
    "            label\n",
    "        )\n",
    "        '''\n",
    "           \n",
    "        5. 加权损失并计算平均值\n",
    "\n",
    "           ```python\n",
    "           weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "           ```\n",
    "\n",
    "           - 将未加权的损失 `unweighted_loss` 与权重张量 `weights` 元素相乘，\n",
    "           屏蔽掉无效部分的损失。\n",
    "           - 对每个样本的时间步维度（`num_steps`）求平均，得到每个样本的加权损失。\n",
    "        '''\n",
    "        weighted_loss = (\n",
    "            unweighted_loss * weights\n",
    "        ).mean(dim=1)\n",
    "        return weighted_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b2d326b-b44c-46be-a045-dd38a4ed32b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3026, 1.1513, 0.0000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = MaskedSoftmaxCELoss()\n",
    "loss(\n",
    "    torch.ones(3, 4, 10),\n",
    "    torch.ones(\n",
    "        (3, 4), dtype=torch.long\n",
    "    ),\n",
    "    torch.tensor([4, 2, 0])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61e9169-890d-44c4-b12c-0c13d80d803c",
   "metadata": {},
   "source": [
    "#### 示例代码\n",
    "\n",
    "```python\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "result = loss(torch.ones(3, 4, 10), torch.ones((3, 4), dtype=torch.long), torch.tensor([4, 2, 0]))\n",
    "print(result)\n",
    "```\n",
    "\n",
    "假设 `torch.ones(3, 4, 10)` 是预测张量 `pred`，`torch.ones((3, 4), dtype=torch.long)` 是标签张量 `label`，`torch.tensor([4, 2, 0])` 是有效长度 `valid_len`。\n",
    "\n",
    "1. **输入张量**\n",
    "\n",
    "   - `pred`：形状为 `(3, 4, 10)` 的全1张量，表示批量大小为3，每个序列长度为4，每个词元的词汇表大小为10。\n",
    "   - `label`：形状为 `(3, 4)` 的全1张量，表示每个词元的标签索引为1。\n",
    "   - `valid_len`：形状为 `(3,)`，值为 `[4, 2, 0]`，表示三个序列的有效长度分别为4、2和0。\n",
    "\n",
    "2. **权重张量**\n",
    "\n",
    "   ```python\n",
    "   weights = sequence_mask(weights, valid_len)\n",
    "   ```\n",
    "\n",
    "   - `weights` 初始为全1张量，形状为 `[3, 4]`。\n",
    "   - 应用 `sequence_mask` 后，`weights` 变为：\n",
    "\n",
    "     ```\n",
    "     tensor([[1, 1, 1, 1],\n",
    "             [1, 1, 0, 0],\n",
    "             [0, 0, 0, 0]])\n",
    "     ```\n",
    "\n",
    "3. **未加权的交叉熵损失**\n",
    "\n",
    "   假设未加权的交叉熵损失 `unweighted_loss` 计算后是形状为 `[3, 4]` 的张量。由于 `pred` 和 `label` 都是全1张量，预测和标签完全一致，损失为某个常数值（例如 `2.3026`，对应 `-log(1/10)`）。\n",
    "\n",
    "   ```\n",
    "   unweighted_loss:\n",
    "   tensor([[2.3026, 2.3026, 2.3026, 2.3026],\n",
    "           [2.3026, 2.3026, 2.3026, 2.3026],\n",
    "           [2.3026, 2.3026, 2.3026, 2.3026]])\n",
    "   ```\n",
    "\n",
    "4. **加权损失**\n",
    "\n",
    "   ```python\n",
    "   weighted_loss = (unweighted_loss * weights).mean(dim=1)\n",
    "   ```\n",
    "\n",
    "   - 加权后的损失：\n",
    "\n",
    "     ```\n",
    "     tensor([[2.3026, 2.3026, 2.3026, 2.3026],\n",
    "             [2.3026, 2.3026, 0.0000, 0.0000],\n",
    "             [0.0000, 0.0000, 0.0000, 0.0000]])\n",
    "     ```\n",
    "\n",
    "   - 按时间步维度求平均：\n",
    "\n",
    "     ```\n",
    "     tensor([2.3026, 1.1513, 0.0000])\n",
    "     ```\n",
    "\n",
    "5. **结果**\n",
    "\n",
    "   最终结果为每个序列的加权损失：\n",
    "\n",
    "   ```python\n",
    "   tensor([2.3026, 1.1513, 0.0000])\n",
    "   ```\n",
    "\n",
    "   解释：\n",
    "   - 第一个序列的有效长度为4，所有时间步的损失都有效，平均损失为 `2.3026`。\n",
    "   - 第二个序列的有效长度为2，只有前两个时间步的损失有效，平均损失为 `1.1513`（即 `2.3026 / 2`）。\n",
    "   - 第三个序列的有效长度为0，所有时间步的损失都无效，平均损失为 `0.0000`。\n",
    "\n",
    "### 总结\n",
    "\n",
    "- `MaskedSoftmaxCELoss` 类通过继承 `nn.CrossEntropyLoss` 并重写 `forward` 方法，实现了对填充词元的遮蔽。\n",
    "- 使用 `sequence_mask` 函数生成遮蔽掩码，避免填充词元对损失计算的干扰。\n",
    "- 通过示例代码展示了带遮蔽损失计算的具体过程，验证了不同有效长度下损失的计算结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca5738-680e-4047-abe6-d12ad4f33488",
   "metadata": {},
   "source": [
    "训练\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcc3ab-d2a4-4ff5-a3c0-e386a2d6ef6a",
   "metadata": {
    "tags": []
   },
   "source": [
    "强制教学（teacher forcing）是一种训练序列到序列模型的方法。在这种方法中，解码器在训练时每一步的输入都是由真实的目标序列提供，而不是由解码器前一步的预测提供。让我们详细解释这个概念。\n",
    "\n",
    "### 强制教学的目的\n",
    "\n",
    "在训练序列到序列模型（例如机器翻译模型）时，解码器需要在每个时间步生成下一个词元。在训练期间，如果每一步都使用解码器前一步的预测作为输入，错误可能会不断累积，导致模型难以学习正确的序列生成方式。为了解决这个问题，可以使用强制教学。\n",
    "\n",
    "### 强制教学的过程\n",
    "\n",
    "1. **输入序列**：假设我们的输入序列是 `X`。\n",
    "2. **目标序列**：目标序列是我们希望模型生成的正确输出 `Y`。\n",
    "3. **初始化解码器**：解码器的输入序列以特殊的开始标记 `<bos>`（beginning of sequence）开始。\n",
    "4. **每一步输入**：\n",
    "   - 在训练过程中，解码器的每一步输入由目标序列提供，而不是使用解码器上一步的输出。这意味着在解码器的第一个时间步，输入是 `<bos>`，在第二个时间步，输入是目标序列的第一个词元，以此类推。\n",
    "   \n",
    "### 代码中的强制教学\n",
    "\n",
    "在代码中，我们看到以下部分：\n",
    "\n",
    "```python\n",
    "bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n",
    "dec_input = torch.cat([bos, Y[:, :-1]], 1)  # 强制教学\n",
    "```\n",
    "\n",
    "1. **创建 `<bos>` 标记**：\n",
    "   ```python\n",
    "   bos = torch.tensor([tgt_vocab['<bos>']] * Y.shape[0], device=device).reshape(-1, 1)\n",
    "   ```\n",
    "   这行代码创建了一个张量 `bos`，其中包含批量大小个 `<bos>` 标记，每个标记作为解码器输入序列的第一个词元。\n",
    "\n",
    "2. **拼接输入序列**：\n",
    "   ```python\n",
    "   dec_input = torch.cat([bos, Y[:, :-1]], 1)\n",
    "   ```\n",
    "   这里将 `<bos>` 标记和目标序列 `Y` 的前 `n-1` 个词元拼接起来，形成解码器的输入序列 `dec_input`。这样，解码器的输入序列就变成了 `[<bos>, Y[0], Y[1], ..., Y[n-2]]`，这就是强制教学的实现方式。\n",
    "\n",
    "### 强制教学的优势\n",
    "\n",
    "1. **减少错误传播**：使用真实的目标序列作为输入，减少了模型由于自身错误预测而导致的误差累积。\n",
    "2. **更快收敛**：模型可以更快地学会正确的序列生成，因为它总是看到正确的输入和输出对。\n",
    "\n",
    "### 总结\n",
    "\n",
    "强制教学是通过使用目标序列中的真实词元作为解码器每一步的输入来训练序列到序列模型的方法。这可以帮助模型更快地学习并减少训练期间的错误传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cdbef82-775c-49fb-bf83-080ed31f44d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(\n",
    "    net, data_iter, lr, num_epochs, tgt_vocab, device\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    训练序列到序列模型\n",
    "    \n",
    "    初始化和准备工作\n",
    "    \n",
    "    1. 权重初始化函数\n",
    "    ```python\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    ```\n",
    "    - 这个函数用于初始化模型中的线性层和GRU层的权重。\n",
    "    - `nn.init.xavier_uniform_` 是一种常用的初始化方法，使得网络更容易训练。\n",
    "\n",
    "    '''\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(\n",
    "                        m._parameters[param]\n",
    "                    )\n",
    "                    \n",
    "    '''\n",
    "     \n",
    "    2. 应用权重初始化并将模型移至设备（GPU或CPU）\n",
    "        ```python\n",
    "        net.apply(xavier_init_weights)\n",
    "        net.to(device)\n",
    "        ```\n",
    "        - `net.apply` 将初始化函数应用到模型的所有子模块上。\n",
    "        - `net.to(device)` 将模型移动到指定设备（例如GPU）上，以加速训练。\n",
    "\n",
    "    '''\n",
    "    net.apply(xavier_init_weights)\n",
    "    net.to(device)\n",
    "    \n",
    "    '''\n",
    "    3. 设置优化器和损失函数\n",
    "        ```python\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "        loss = MaskedSoftmaxCELoss()\n",
    "        ```\n",
    "        - 使用Adam优化器进行梯度下降。\n",
    "        - 使用之前定义的带遮蔽的softmax交叉熵损失函数。\n",
    "\n",
    "    '''\n",
    "    optimizer = torch.optim.Adam(\n",
    "        net.parameters(),\n",
    "        lr=lr\n",
    "    )\n",
    "    loss = MaskedSoftmaxCELoss()\n",
    "    '''\n",
    "    4. 设置动画器\n",
    "        ```python\n",
    "        animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[10, num_epochs])\n",
    "        ```\n",
    "        - 用于可视化训练过程中的损失变化。\n",
    "\n",
    "    '''\n",
    "    # 进入训练模式\n",
    "    net.train()\n",
    "    animator = d2l.Animator(\n",
    "        xlabel='epoch',\n",
    "        ylabel='loss',\n",
    "        xlim=[10, num_epochs]\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        # 训练损失总和，词元数量\n",
    "        metric = d2l.Accumulator(2) \n",
    "\n",
    "        for batch in data_iter:\n",
    "            optimizer.zero_grad() # 清除梯度。\n",
    "            # 从批量数据中获取输入和输出序列及其有效长度。\n",
    "            X, X_valid_len, Y, Y_valid_len = [\n",
    "                x.to(device) for x in batch\n",
    "            ]\n",
    "            '''\n",
    "            创建一个序列开始词元`<bos>`\n",
    "            \n",
    "            1. `tgt_vocab['<bos>']`\n",
    "            首先，`tgt_vocab['<bos>']` 是获取\n",
    "            目标词汇表（target vocabulary）中 `<bos>` 标记对应的索引。\n",
    "            假设 `<bos>` 在词汇表中的索引是 1，那么 `tgt_vocab['<bos>']` 就是 1。\n",
    "            \n",
    "            2. `[tgt_vocab['<bos>']] * Y.shape[0]`\n",
    "\n",
    "            假设 `Y.shape[0]` 是批量大小（batch size）。\n",
    "            例如，批量大小为 4，那么 `Y.shape[0]` 就是 4。\n",
    "            `[tgt_vocab['<bos>']] * Y.shape[0]` \n",
    "            会生成一个包含 4 个 `<bos>` 索引的列表。比如，`[1, 1, 1, 1]`。\n",
    "            \n",
    "            3. `.reshape(-1, 1)`\n",
    "\n",
    "            最后，`.reshape(-1, 1)` 会把张量的形状从 `[4]` 转换为 `[4, 1]`，\n",
    "            即每个 `<bos>` 索引变成一个单独的行：\n",
    "\n",
    "            ```python\n",
    "            tensor([[1],\n",
    "                    [1],\n",
    "                    [1],\n",
    "                    [1]], device='cuda:0')\n",
    "            ```\n",
    "\n",
    "            '''\n",
    "            bos = torch.tensor(\n",
    "                [tgt_vocab['<bos>']] * Y.shape[0],\n",
    "                device=device\n",
    "            ).reshape(-1, 1)\n",
    "            '''\n",
    "            将`<bos>`和原始输出序列拼接作为解码器的输入，\n",
    "            这被称为“强制教学”。\n",
    "            '''\n",
    "            dec_input = torch.cat(\n",
    "                [bos, Y[:, :-1]], 1\n",
    "            )\n",
    "            # 获取模型的预测输出\n",
    "            Y_hat, _ = net(X, dec_input, X_valid_len)\n",
    "            l = loss(Y_hat, Y, Y_valid_len)\n",
    "            # 损失函数的标量进行‘反向传播’\n",
    "            l.sum().backward()\n",
    "            # 裁剪梯度以防止梯度爆炸\n",
    "            d2l.grad_clipping(net, 1)\n",
    "            num_tokens = Y_valid_len.sum()\n",
    "            # 更新模型参数\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                # 累加损失和词元数量\n",
    "                metric.add(l.sum(), num_tokens)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.add(\n",
    "                epoch + 1,\n",
    "                (metric[0] / metric[1],)\n",
    "            )\n",
    "            \n",
    "    print(\n",
    "        f'loss {metric[0] / metric[1]:.3f}, ', \n",
    "        f'{metric[1] / timer.stop():.1f}, ',\n",
    "        f'tokens/sec on {str(device)}'\n",
    "    )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f88c4f-6cfb-4cb0-a80f-2bb0a0c21a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c416293-ab79-4034-8e22-e52c7376dddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "199b18b0-8fc5-43fa-8ab0-908d907e231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64,10\n",
    "lr, num_epochs, device = 0.005, 300, d2l.try_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81f69fce-2021-406d-b617-f3f784e90e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.019,  2747.7,  tokens/sec on cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n总结\\n\\n- **编码器**：\\n将输入序列编码成隐状态。\\n\\n- **解码器**：\\n使用编码器的隐状态和强制教学进行序列生成。\\n\\n- **训练循环**：\\n通过梯度下降优化模型参数，使用带遮蔽的交叉熵损失函数来计算损失。\\n\\n- **强制教学**：\\n在解码过程中使用真实的目标序列来指导模型的学习。\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 262.1875 180.65625\" width=\"262.1875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2024-06-26T14:01:37.151642</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.4.3, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 180.65625 \n",
       "L 262.1875 180.65625 \n",
       "L 262.1875 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 50.14375 143.1 \n",
       "L 245.44375 143.1 \n",
       "L 245.44375 7.2 \n",
       "L 50.14375 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 77.081681 143.1 \n",
       "L 77.081681 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"ma80d2eb6ff\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"77.081681\" xlink:href=\"#ma80d2eb6ff\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 50 -->\n",
       "      <g transform=\"translate(70.719181 157.698438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n",
       "        <path d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 110.754095 143.1 \n",
       "L 110.754095 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"110.754095\" xlink:href=\"#ma80d2eb6ff\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(101.210345 157.698438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 144.426509 143.1 \n",
       "L 144.426509 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"144.426509\" xlink:href=\"#ma80d2eb6ff\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 150 -->\n",
       "      <g transform=\"translate(134.882759 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 178.098922 143.1 \n",
       "L 178.098922 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.098922\" xlink:href=\"#ma80d2eb6ff\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(168.555172 157.698438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 211.771336 143.1 \n",
       "L 211.771336 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"211.771336\" xlink:href=\"#ma80d2eb6ff\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 250 -->\n",
       "      <g transform=\"translate(202.227586 157.698438)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 245.44375 143.1 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"245.44375\" xlink:href=\"#ma80d2eb6ff\" y=\"143.1\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(235.9 157.698438)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_7\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(132.565625 171.376563)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n",
       "       <path d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n",
       "       <path d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n",
       "       <path d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n",
       "       <path d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use x=\"61.523438\" xlink:href=\"#DejaVuSans-70\"/>\n",
       "      <use x=\"125\" xlink:href=\"#DejaVuSans-6f\"/>\n",
       "      <use x=\"186.181641\" xlink:href=\"#DejaVuSans-63\"/>\n",
       "      <use x=\"241.162109\" xlink:href=\"#DejaVuSans-68\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 50.14375 116.833449 \n",
       "L 245.44375 116.833449 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"m83860e1a6e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m83860e1a6e\" y=\"116.833449\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 0.05 -->\n",
       "      <g transform=\"translate(20.878125 120.632667)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 50.14375 84.728307 \n",
       "L 245.44375 84.728307 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m83860e1a6e\" y=\"84.728307\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 0.10 -->\n",
       "      <g transform=\"translate(20.878125 88.527526)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 50.14375 52.623165 \n",
       "L 245.44375 52.623165 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m83860e1a6e\" y=\"52.623165\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 0.15 -->\n",
       "      <g transform=\"translate(20.878125 56.422384)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 50.14375 20.518024 \n",
       "L 245.44375 20.518024 \n",
       "\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m83860e1a6e\" y=\"20.518024\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 0.20 -->\n",
       "      <g transform=\"translate(20.878125 24.317243)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-30\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_12\">\n",
       "     <!-- loss -->\n",
       "     <g transform=\"translate(14.798437 84.807812)rotate(-90)scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n",
       "       <path d=\"M 2834 3397 \n",
       "L 2834 2853 \n",
       "Q 2591 2978 2328 3040 \n",
       "Q 2066 3103 1784 3103 \n",
       "Q 1356 3103 1142 2972 \n",
       "Q 928 2841 928 2578 \n",
       "Q 928 2378 1081 2264 \n",
       "Q 1234 2150 1697 2047 \n",
       "L 1894 2003 \n",
       "Q 2506 1872 2764 1633 \n",
       "Q 3022 1394 3022 966 \n",
       "Q 3022 478 2636 193 \n",
       "Q 2250 -91 1575 -91 \n",
       "Q 1294 -91 989 -36 \n",
       "Q 684 19 347 128 \n",
       "L 347 722 \n",
       "Q 666 556 975 473 \n",
       "Q 1284 391 1588 391 \n",
       "Q 1994 391 2212 530 \n",
       "Q 2431 669 2431 922 \n",
       "Q 2431 1156 2273 1281 \n",
       "Q 2116 1406 1581 1522 \n",
       "L 1381 1569 \n",
       "Q 847 1681 609 1914 \n",
       "Q 372 2147 372 2553 \n",
       "Q 372 3047 722 3315 \n",
       "Q 1072 3584 1716 3584 \n",
       "Q 2034 3584 2315 3537 \n",
       "Q 2597 3491 2834 3397 \n",
       "z\n",
       "\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\"/>\n",
       "      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n",
       "      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n",
       "      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_21\">\n",
       "    <path clip-path=\"url(#p7fd9feb4e5)\" d=\"M 50.14375 13.377273 \n",
       "L 56.878233 56.082208 \n",
       "L 63.612716 80.15627 \n",
       "L 70.347198 95.728072 \n",
       "L 77.081681 106.208718 \n",
       "L 83.816164 114.004964 \n",
       "L 90.550647 119.780058 \n",
       "L 97.285129 122.82634 \n",
       "L 104.019612 126.989771 \n",
       "L 110.754095 129.362875 \n",
       "L 117.488578 131.23393 \n",
       "L 124.22306 132.326429 \n",
       "L 130.957543 133.259035 \n",
       "L 137.692026 133.770895 \n",
       "L 144.426509 134.599115 \n",
       "L 151.160991 134.435626 \n",
       "L 157.895474 135.31513 \n",
       "L 164.629957 134.935096 \n",
       "L 171.36444 135.634887 \n",
       "L 178.098922 135.835721 \n",
       "L 184.833405 136.070259 \n",
       "L 191.567888 136.355579 \n",
       "L 198.302371 136.294265 \n",
       "L 205.036853 136.274871 \n",
       "L 211.771336 136.682035 \n",
       "L 218.505819 136.631447 \n",
       "L 225.240302 136.808818 \n",
       "L 231.974784 136.922727 \n",
       "L 238.709267 136.812525 \n",
       "L 245.44375 136.82382 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 50.14375 143.1 \n",
       "L 50.14375 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 245.44375 143.1 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 50.14375 143.1 \n",
       "L 245.44375 143.1 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 50.14375 7.2 \n",
       "L 245.44375 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p7fd9feb4e5\">\n",
       "   <rect height=\"135.9\" width=\"195.3\" x=\"50.14375\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)\n",
    "encoder = Seq2SeqEncoder(\n",
    "    len(src_vocab),\n",
    "    embed_size,\n",
    "    num_hiddens,\n",
    "    num_layers,\n",
    "    dropout\n",
    ")\n",
    "decoder = Seq2SeqDecoder(\n",
    "    len(tgt_vocab),\n",
    "    embed_size,\n",
    "    num_hiddens,\n",
    "    num_layers,\n",
    "    dropout\n",
    ")\n",
    "net = d2l.EncoderDecoder(encoder, decoder)\n",
    "train_seq2seq(net, train_iter, lr, num_epochs, tgt_vocab, device)\n",
    "\n",
    "'''\n",
    "总结\n",
    "\n",
    "- **编码器**：\n",
    "将输入序列编码成隐状态。\n",
    "\n",
    "- **解码器**：\n",
    "使用编码器的隐状态和强制教学进行序列生成。\n",
    "\n",
    "- **训练循环**：\n",
    "通过梯度下降优化模型参数，使用带遮蔽的交叉熵损失函数来计算损失。\n",
    "\n",
    "- **强制教学**：\n",
    "在解码过程中使用真实的目标序列来指导模型的学习。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6931b8-93d3-4571-b775-6f68421f9785",
   "metadata": {},
   "source": [
    "预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c62ff049-e72c-494b-a076-f5e73e2dcd69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n这段代码实现了一个序列到序列模型的预测过程，通过循环迭代逐步生成输出序列，\\n并在预测结束标记`<eos>`时停止。\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict_seq2seq(\n",
    "    net, \n",
    "    src_sentence, \n",
    "    sec_vocab, \n",
    "    tgt_vocab, \n",
    "    num_steps, \n",
    "    device, \n",
    "    save_attention_weights=False\n",
    "):\n",
    "    '''\n",
    "    序列到序列模型的预测\n",
    "    在预测时将net设置为评估模式\n",
    "    这会禁用dropout层和其他仅在训练时使用的层。\n",
    "    \n",
    "    \n",
    "    准备输入序列\n",
    "\n",
    "    1. Token化和转为小写：将输入句子转为小写并拆分为单词。\n",
    "    2. 转为索引：使用`src_vocab`将这些单词转为对应的词汇表索引。\n",
    "    3. 添加`<eos>`标记：在序列末尾添加结束标记`<eos>`。\n",
    "    '''\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[\n",
    "        src_sentence.lower().split(' ')\n",
    "    ] + [\n",
    "        src_vocab['<eos>']\n",
    "    ]\n",
    "    '''\n",
    "    获取有效长度\n",
    "\n",
    "    将源序列的长度存储在`enc_valid_len`中，以便在编码器中使用。\n",
    "    '''\n",
    "    enc_valid_len = torch.tensor(\n",
    "        [len(src_tokens)],\n",
    "        device=device\n",
    "    )\n",
    "    '''\n",
    "    截断或填充序列\n",
    "\n",
    "    如果源序列的长度超过`num_steps`，则截断序列；否则，用`<pad>`标记填充，使其长度等于\n",
    "    '''\n",
    "    src_tokens = d2l.truncate_pad(\n",
    "        src_tokens, num_steps, src_vocab['<pad>']\n",
    "    )\n",
    "    '''\n",
    "    添加批量轴\n",
    "\n",
    "    将序列转换为张量，并在第一个维度上添加一个批量轴，使其形状变为`(1, num_steps)`。\n",
    "    '''\n",
    "    enc_X = torch.unsqueeze(\n",
    "        torch.tensor(\n",
    "            src_tokens, dtype=torch.long, device=device\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    '''\n",
    "    编码器前向传播\n",
    "\n",
    "    将预处理后的源序列输入编码器，获得编码器的输出`enc_outputs`。\n",
    "    '''\n",
    "    enc_outputs = net.encoder(enc_X, enc_valid_len)\n",
    "    '''\n",
    "    初始化解码器状态\n",
    "\n",
    "    使用编码器的输出初始化解码器的隐藏状态。\n",
    "    '''\n",
    "    dec_state = net.decoder.init_state(enc_outputs, enc_valid_len)\n",
    "    '''\n",
    "    添加批量轴\n",
    "    \n",
    "    初始化解码器输入\n",
    "\n",
    "    创建解码器的初始输入（`<bos>`标记），并在第一个维度上添加批量轴。\n",
    "\n",
    "    '''\n",
    "    dec_X = torch.unsqueeze(\n",
    "        torch.tensor(\n",
    "            [tgt_vocab['<bos>']],\n",
    "            dtype=torch.long, device=device\n",
    "        ),\n",
    "        dim=0\n",
    "    )\n",
    "    '''\n",
    "    初始化输出序列和注意力权重序列\n",
    "\n",
    "    准备空列表来存储生成的输出序列和注意力权重（如果需要保存注意力权重）。\n",
    "    '''\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    \n",
    "    for _ in range(num_steps):\n",
    "        '''\n",
    "        解码器的前向传播和预测\n",
    "\n",
    "        1. 前向传播：在每个时间步，使用解码器进行前向传播，\n",
    "        输入是`dec_X`（初始为`<bos>`，后续为上一步的预测）。\n",
    "        \n",
    "        2. 选择最高概率的词元：使用`argmax`选择预测概率最高的词元作为下一个时间步的输入。\n",
    "        \n",
    "        3. 去除批量轴并获取预测词元：将预测词元从张量转换为整数索引。\n",
    "\n",
    "        '''\n",
    "        Y, dec_state = net.decoder(dec_X, dec_state)\n",
    "        # 我们使用具有预测最高可能性的词元，作为解码器在下一时间步的输入\n",
    "        dec_X = Y.argmax(dim=2)\n",
    "        pred = dec_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        '''\n",
    "        保存注意力权重\n",
    "        如果`save_attention_weights`为True，则保存当前时间步的注意力权重。\n",
    "        '''\n",
    "        if save_attention_weights:\n",
    "            attention_weight_seq.append(\n",
    "                net.decoder.attention_weights\n",
    "            )\n",
    "        # 一旦序列结束词元被预测，输出序列的生成就完成了\n",
    "        '''\n",
    "        生成输出序列\n",
    "\n",
    "        1. 检查结束标记：如果预测词元是`<eos>`，则停止生成。\n",
    "        2. 保存预测词元：将预测词元添加到输出序列中。\n",
    "\n",
    "        '''\n",
    "        if pred == tgt_vocab['<eos>']:\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    '''\n",
    "    返回结果\n",
    "\n",
    "    将输出序列的索引转换为对应的词元，并连接成字符串返回。\n",
    "    如果有保存注意力权重的需求，也返回注意力权重序列。\n",
    "    '''\n",
    "    return ' '.join(\n",
    "        tgt_vocab.to_tokens(output_seq)\n",
    "    ), attention_weight_seq\n",
    "\n",
    "\n",
    "'''\n",
    "这段代码实现了一个序列到序列模型的预测过程，通过循环迭代逐步生成输出序列，\n",
    "并在预测结束标记`<eos>`时停止。\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30257e7-cc41-4680-a9d5-8ecf1a65cd24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb78cb5a-e50f-4f89-b6c3-8791bef3e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BLEU算法（例子和公式解释）\n",
    "\n",
    "https://blog.csdn.net/qq_30232405/article/details/104219396\n",
    "'''\n",
    "\n",
    "def bleu(pred_seq, label_seq, k):\n",
    "    # 计算BLEU\n",
    "    \n",
    "    '''\n",
    "    1. 输入和分词：\n",
    "    - `pred_seq`和`label_seq`分别是预测序列和标签序列（真实序列）。\n",
    "    - 用`split(' ')`将它们分割成词元列表：`pred_tokens`和`label_tokens`。\n",
    "    - 获取预测序列和标签序列的长度：`len_pred`和`len_label`。\n",
    "\n",
    "    '''\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    '''\n",
    "    2. 长度惩罚：\n",
    "    - 计算长度惩罚，防止过短的预测序列获得过高的分数。\n",
    "    `math.exp(min(0, 1 - len_label / len_pred))`会在\n",
    "    `len_pred`小于`len_label`时对分数进行惩罚。\n",
    "\n",
    "    '''\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(\n",
    "        min(0, 1-len_label / len_pred)\n",
    "    )\n",
    "    '''\n",
    "    3. n元语法匹配计数：\n",
    "    - `k`是最大的n-gram长度。在这里，我们会依次计算1-gram，2-gram，…，k-gram的匹配情况。\n",
    "    - 初始化匹配计数`num_matches`和一个字典`label_subs`来存储标签序列中的n-gram计数。\n",
    "    \n",
    "    collections.defaultdict 是 Python 标准库中的一个字典子类。\n",
    "    与普通字典不同，defaultdict 在试图访问不存在的键时不会引发 KeyError 异常，而是会自动为这个键生成一个默认值。\n",
    "\n",
    "    在 collections.defaultdict(int) 中，int 是默认工厂函数（即用来生成默认值的函数）。\n",
    "    在这种情况下，int() 返回 0，因此 defaultdict(int) 会在访问不存在的键时自动创建该键，\n",
    "    并将其默认值设置为 0。\n",
    "    '''\n",
    "    for  n in range(1, k+1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        '''\n",
    "        4. 标签n元语法计数：\n",
    "        - 对于每个n-gram长度，计算标签序列中的所有n-gram，并在`label_subs`中记录其出现次数。\n",
    "        \n",
    "        第1次循环：\n",
    "        label_subs: defaultdict(<class 'int'>, {'va': 1, '!': 1})\n",
    "        label_subs: defaultdict(<class 'int'>, {'va !': 1})\n",
    "        \n",
    "        第2次循环：\n",
    "        label_subs: defaultdict(<class 'int'>, {\"j'ai\": 1, 'perdu': 1, '.': 1})\n",
    "        label_subs: defaultdict(<class 'int'>, {\"j'ai perdu\": 1, 'perdu .': 1})\n",
    "        \n",
    "        第3次循环：\n",
    "        label_subs: defaultdict(<class 'int'>, {'il': 1, 'est': 1, 'calme': 1, '.': 1})\n",
    "        label_subs: defaultdict(<class 'int'>, {'il est': 1, 'est calme': 1, 'calme .': 1})\n",
    "        \n",
    "        第4次循环：\n",
    "        label_subs: defaultdict(<class 'int'>, {'je': 1, 'suis': 1, 'chez': 1, 'moi': 1, '.': 1})\n",
    "        label_subs: defaultdict(<class 'int'>, {'je suis': 1, 'suis chez': 1, 'chez moi': 1, 'moi .': 1})\n",
    "        '''\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        '''\n",
    "        5. 预测n元语法匹配：\n",
    "        - 对于预测序列中的每个n-gram，如果在`label_subs`中找到匹配，则增加匹配计数，\n",
    "        并减少`label_subs`中的计数，以避免重复匹配。\n",
    "\n",
    "        '''\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i+n])] -= 1\n",
    "        '''\n",
    "        6. 计算n元语法的精确度：\n",
    "        - 计算n-gram的精确度（即匹配的n-gram数与预测序列中n-gram总数的比值）。\n",
    "        - 用`math.pow(0.5, n)`加权精确度，较长的n-gram权重更大。\n",
    "\n",
    "        '''\n",
    "        score *= math.pow(\n",
    "            num_matches / (len_pred - n + 1),\n",
    "            math.pow(0.5, n)\n",
    "        )\n",
    "    return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05db10bc-8226-4625-b79c-fe8dac9e4d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go . => va ! bleu 1.000\n",
      "i lost . => j'ai perdu . bleu 1.000\n",
      "he's calm . => il est <unk> . bleu 0.658\n",
      "i'm home . => je suis chez moi chez nous . bleu 0.711\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "- `engs`是待翻译的英语句子。\n",
    "- `fras`是对应的法语标签句子。\n",
    "'''\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "\n",
    "for eng, fra in zip(engs, fras):\n",
    "    translation, attention_weight_seq = predict_seq2seq(\n",
    "        net, eng, src_vocab, tgt_vocab, num_steps, device\n",
    "    )\n",
    "    print(\n",
    "        f'{eng} => {translation}',\n",
    "        f'bleu {bleu(translation, fra, k=2):.3f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd0d1a1-68c0-4ff1-9304-01c90c9f28d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67030ed9-c116-4390-89d7-9fd5015018cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
