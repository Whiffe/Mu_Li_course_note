{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ffdb4f-393d-4fed-b7e4-360a602306aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9ab10-5ad2-4cec-9f5a-f3658fee8149",
   "metadata": {},
   "source": [
    "输入表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538ca9db-c36f-402a-a009-b16d1500b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "这段代码的主要功能是生成BERT模型的输入序列。\n",
    "它会将单个文本或文本对转换成BERT输入序列，\n",
    "并生成相应的片段索引。\n",
    "'''\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"\n",
    "    获取输入序列的词元及其片段索引\n",
    "    \n",
    "    1. 单个文本输入\n",
    "    如果只输入一个文本序列 `tokens_a`，\n",
    "    函数会在序列的开头加上一个特殊词元 `<cls>`，\n",
    "    在序列的结尾加上一个特殊词元 `<sep>`，并生成对应的片段索引。\n",
    "    \n",
    "    - `tokens` 是由特殊词元 `<cls>` 开头，\n",
    "        接着是 `tokens_a` 的所有词元，最后加上 `<sep>`。\n",
    "\n",
    "    \"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    '''\n",
    "    0和1分别标记片段A和B\n",
    "    \n",
    "    - `segments` 是一个全0的列表，长度与 `tokens` 相同，\n",
    "        表示这些词元都属于片段A。\n",
    "    '''\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    '''\n",
    "    2. 文本对输入\n",
    "    \n",
    "    如果输入两个文本序列 `tokens_a` 和 `tokens_b`，\n",
    "    函数会将两个序列连接起来，中间和结尾加上 `<sep>`，\n",
    "    并生成对应的片段索引。\n",
    "    '''\n",
    "    if tokens_b is not None:\n",
    "        '''\n",
    "        - `tokens` 是由特殊词元 `<cls>` 开头，\n",
    "            接着是 `tokens_a` 的所有词元，\n",
    "            然后是 `<sep>`，再接着是 `tokens_b` 的所有词元，\n",
    "            最后再加上一个 `<sep>`。\n",
    "        - `segments` 在 `tokens_a` 部分是全0的列表，\n",
    "            在 `tokens_b` 部分是全1的列表，\n",
    "            表示 `tokens_a` 属于片段A，`tokens_b` 属于片段B。\n",
    "        '''\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d0f3110-994a-4e39-babf-c9e06642043a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<cls>', 'hello', ',', 'world', '!', '<sep>', 'how', 'are', 'you', '?', '<sep>']\n",
      "[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tokens_a = ['hello', ',', 'world', '!']\n",
    "tokens_b = ['how', 'are', 'you', '?']\n",
    "tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "print(tokens)    # ['<cls>', 'hello', ',', 'world', '!', '<sep>', 'how', 'are', 'you', '?', '<sep>']\n",
    "print(segments)  # [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e425c4c-2e10-42e0-94b9-e39ebe2ced9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "BERT（Bidirectional Encoder Representations from Transformers）\n",
    "是一种强大的自然语言处理模型，它使用了Transformer架构的一部分（编码器部分），\n",
    "并增加了片段嵌入和可学习的位置嵌入。\n",
    "'''\n",
    "class BERTEncoder(nn.Module):\n",
    "    '''\n",
    "    - `vocab_size`：词汇表的大小，即模型可以处理的不同词元的数量。\n",
    "    - `num_hiddens`：隐藏单元的数量，表示每个词元的嵌入向量的长度。\n",
    "    - `norm_shape`：规范化层的形状。\n",
    "    - `ffn_num_input` 和 `ffn_num_hiddens`：\n",
    "        前馈神经网络的输入和隐藏单元数量。\n",
    "    - `num_heads`：多头注意力机制中的头数。\n",
    "    - `num_layers`：编码器层的数量。\n",
    "    - `dropout`：Dropout概率，用于防止过拟合。\n",
    "    - `max_len`：最大序列长度。\n",
    "    - `key_size`, `query_size`, `value_size`：\n",
    "        注意力机制的键、查询和值的维度。\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,\n",
    "        num_hiddens,\n",
    "        norm_shape,\n",
    "        ffn_num_input,\n",
    "        ffn_num_hiddens,\n",
    "        num_heads,\n",
    "        num_layers,\n",
    "        dropout,\n",
    "        max_len=1000,\n",
    "        key_size=768,\n",
    "        query_size=768,\n",
    "        value_size=768,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        '''\n",
    "        词元嵌入层：将词表中的索引转化为向量表示\n",
    "        - `token_embedding`：词元嵌入层，用于将词元转换为嵌入向量。\n",
    "        '''\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            vocab_size, num_hiddens\n",
    "        )\n",
    "        '''\n",
    "        片段嵌入层：区分不同的文本段落\n",
    "        - `segment_embedding`：片段嵌入层，用于区分两个文本片段。\n",
    "        '''\n",
    "        self.segment_embedding = nn.Embedding(\n",
    "            2, num_hiddens\n",
    "        )\n",
    "        '''\n",
    "        编码器块的容器\n",
    "        - `blks`：编码器块的顺序容器，包含多个编码器层。\n",
    "        '''\n",
    "        self.blks = nn.Sequential()\n",
    "        '''\n",
    "        添加多层编码器块\n",
    "        '''\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(\n",
    "                f'{i}',\n",
    "                d2l.EncoderBlock(\n",
    "                    key_size,\n",
    "                    query_size,\n",
    "                    value_size,\n",
    "                    num_hiddens,\n",
    "                    norm_shape,\n",
    "                    ffn_num_input,\n",
    "                    ffn_num_hiddens,\n",
    "                    num_heads,\n",
    "                    dropout,\n",
    "                    True\n",
    "                )\n",
    "            )\n",
    "        '''\n",
    "        位置嵌入：用来表示输入序列中词元的位置，位置嵌入是可学习的\n",
    "        在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
    "        \n",
    "        - `pos_embedding`：\n",
    "            可学习的位置嵌入参数，用于表示词元在序列中的位置。\n",
    "        '''\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(\n",
    "                1, max_len, num_hiddens\n",
    "            )\n",
    "        )\n",
    "    '''\n",
    "    - `tokens`：输入的词元序列，形状为 `(批量大小, 最大序列长度)`。\n",
    "    - `segments`：片段索引，\n",
    "        形状与 `tokens` 相同，用于区分不同的文本片段。\n",
    "    - `valid_lens`：有效长度，用于处理不同长度的序列。\n",
    "    '''\n",
    "    def forward(self, tokens, segments, valid_lens):\n",
    "        '''\n",
    "        **嵌入层**：\n",
    "        将词元嵌入和片段嵌入相加\n",
    "        将词元和片段索引分别通过嵌入层，并将它们相加。\n",
    "        '''\n",
    "        X = self.token_embedding(tokens) + self.segment_embedding(segments)\n",
    "        '''\n",
    "        加上位置嵌入\n",
    "        将位置嵌入添加到 `X` 中，表示词元在序列中的位置。\n",
    "        '''\n",
    "        X = X + self.pos_embedding.data[:, :X.shape[1], :]\n",
    "        '''\n",
    "        **编码器块**：\n",
    "        通过每个编码器块进行处理\n",
    "        每个块进行自注意力计算和前馈神经网络处理。\n",
    "\n",
    "        '''\n",
    "        for blk in self.blks:\n",
    "            X = blk(X, valid_lens)\n",
    "        '''\n",
    "        **返回值**：\n",
    "        - `X`：编码后的表示，形状为 \n",
    "            `(批量大小, 最大序列长度, num_hiddens)`。\n",
    "\n",
    "        '''\n",
    "        return X\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52d3788b-5536-41e3-ac55-fafd8c8796b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vocab_size 词表大小为10000\n",
    "'''\n",
    "vocab_size, num_hiddens, ffn_num_hiddens, num_heads = 10000, 768, 1024,4\n",
    "norm_shape, ffn_num_input, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(\n",
    "    vocab_size,\n",
    "    num_hiddens,\n",
    "    norm_shape,\n",
    "    ffn_num_input,\n",
    "    ffn_num_hiddens,\n",
    "    num_heads,\n",
    "    num_layers,\n",
    "    dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b96a49c-ee44-4bb4-a5a2-2c3d11dc9492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "我们定义两个长度为8的输入序列，\n",
    "每个序列中的词元是从词表中随机选择的索引。\n",
    "片段标记用于区分输入的不同部分。\n",
    "\n",
    "**生成随机输入**\n",
    "- `tokens`：形状为 `(2, 8)` 的随机词元序列，\n",
    "    每个词元是一个在 `[0, vocab_size)` 范围内的随机整数。\n",
    "- `segments`：形状为 `(2, 8)` 的片段索引，\n",
    "    第一个序列的前四个词元属于片段A（标记为0），\n",
    "    后四个词元属于片段B（标记为1）。\n",
    "'''\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "tokens = torch.randint(0, vocab_size, (2,8))\n",
    "segments = torch.tensor(\n",
    "    [[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]]\n",
    ")\n",
    "'''\n",
    "**编码输入数据**：\n",
    "我们将输入的 `tokens` 和 `segments` 传入 BERT 编码器进行前向推断，\n",
    "并输出编码结果。每个词元都会被表示为一个长度为 `num_hiddens` 的向量。\n",
    "'''\n",
    "encoded_X = encoder(\n",
    "    tokens, segments, None\n",
    ")\n",
    "'''\n",
    "输出的形状为 `[2, 8, 768]`，表示：\n",
    "- 我们有2个输入序列\n",
    "- 每个输入序列有8个词元\n",
    "- 每个词元被表示为一个768维的向量\n",
    "\n",
    "\n",
    "3. **输出形状**：\n",
    "    - `encoded_X.shape`：显示编码结果的形状为 `(2, 8, 768)`，表示有2个序列，每个序列长度为8，每个词元被编码为长度为768的向量。\n",
    "\n",
    "\n",
    "'''\n",
    "encoded_X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e951f47-0a86-44aa-9080-a322ae9035fb",
   "metadata": {},
   "source": [
    "- **词元嵌入层** 将词表中的索引转化为向量表示。\n",
    "- **片段嵌入层** 用于区分不同的文本段落。\n",
    "- **位置嵌入** 用于表示输入序列中词元的位置，并且是可学习的。\n",
    "- **编码器块** 是由多个Transformer编码器块组成的，每个编码器块都会对输入数据进行处理。\n",
    "\n",
    "通过这种方式，BERT能够处理单文本输入和文本对输入，并为每个词元生成一个高维的向量表示，这些表示可以用于下游的各种自然语言处理任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d506c-3fa2-4bba-bb24-207d3c174fdd",
   "metadata": {},
   "source": [
    "---\n",
    "预训练任务\n",
    "\n",
    "掩蔽语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6898608e-b9b7-4efc-af7e-2ef4b1297f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "掩蔽语言模型（Masked Language Modeling，MLM）任务的类`MaskLM`，\n",
    "用于BERT预训练。\n",
    "'''\n",
    "\n",
    "class MaskLM(nn.Module):\n",
    "    '''\n",
    "    Bert的掩蔽语言模型任务\n",
    "    \n",
    "    - `vocab_size`：词表大小，即模型需要预测的不同词的数量。  \n",
    "    - `num_hiddens`：隐藏层的大小。\n",
    "    - `num_inputs`：输入的特征数量，默认为768。\n",
    "    - `self.mlp`：定义了一个多层感知机（MLP），用于预测掩蔽词元。它包含：\n",
    "        一个线性层，将输入特征数从`num_inputs`（768）变换到`num_hiddens`。\n",
    "        一个ReLU激活函数。\n",
    "        一个LayerNorm层，用于对隐藏层输出进行规范化。\n",
    "        一个线性层，将隐藏层的输出变换到词表大小（`vocab_size`）。\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        vocab_size,\n",
    "        num_hiddens,\n",
    "        num_inputs=768,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_inputs, num_hiddens),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(num_hiddens),\n",
    "            nn.Linear(num_hiddens, vocab_size)\n",
    "        )\n",
    "    '''\n",
    "    - **输入**：\n",
    "      - `X`：来自BERT编码器的输出，\n",
    "          形状为（批量大小，序列长度，隐藏单元数）。\n",
    "      - `pred_positions`：需要预测的掩蔽词元的位置，\n",
    "          形状为（批量大小，预测位置数量）。\n",
    "    '''\n",
    "    def forward(self, X, pred_positions):\n",
    "        '''\n",
    "        - **过程**：\n",
    "        - `num_pred_positions`：预测位置的数量。\n",
    "        '''\n",
    "        num_pred_positions = pred_positions.shape[1]\n",
    "        '''\n",
    "        - `pred_positions = pred_positions.reshape(-1)`：\n",
    "          将预测位置展开为一维向量。\n",
    "        '''\n",
    "        pred_positions = pred_positions.reshape(-1)\n",
    "        '''\n",
    "        - `batch_size = X.shape[0]`：批量大小。\n",
    "        '''\n",
    "        batch_size = X.shape[0]\n",
    "        '''\n",
    "        - `batch_idx = torch.arange(0, batch_size)`：\n",
    "          生成一个包含批量索引的张量。\n",
    "        '''\n",
    "        batch_idx = torch.arange(0, batch_size)\n",
    "        '''\n",
    "        - `batch_idx = torch.repeat_interleave(batch_idx, num_pred_positions)`：\n",
    "          重复批量索引，使得每个批量索引对应每个预测位置。\n",
    "        '''\n",
    "        batch_idx = torch.repeat_interleave(\n",
    "            batch_idx, num_pred_positions\n",
    "        )\n",
    "        '''\n",
    "        - `masked_X = X[batch_idx, pred_positions]`：\n",
    "          从`X`中选出需要预测的位置，\n",
    "          得到的`masked_X`形状为（批量大小 * 预测位置数量，隐藏单元数）。\n",
    "        '''\n",
    "        masked_x = X[batch_idx, pred_positions]\n",
    "        '''\n",
    "        - `masked_X = masked_X.reshape((batch_size, num_pred_positions, -1))`：\n",
    "          重新调整`masked_X`的形状，\n",
    "          使其为（批量大小，预测位置数量，隐藏单元数）。\n",
    "        '''\n",
    "        masked_x = masked_x.reshape(\n",
    "            (batch_size, num_pred_positions, -1)\n",
    "        )\n",
    "        '''\n",
    "        - `mlm_Y_hat = self.mlp(masked_X)`：\n",
    "          通过MLP计算掩蔽位置的预测结果。\n",
    "        '''\n",
    "        mlm_Y_hat = self.mlp(masked_x)\n",
    "        '''\n",
    "        - **输出**：\n",
    "          - `mlm_Y_hat`：预测结果，\n",
    "          形状为（批量大小，预测位置数量，词表大小）。\n",
    "        '''\n",
    "        return mlm_Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5be3f0bd-9f58-4384-8312-cdb1ceea128a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "实例化和前向推断\n",
    "\n",
    "- **实例化**：创建`MaskLM`的实例，\n",
    "    `vocab_size`和`num_hiddens`分别为词表大小和隐藏单元数。\n",
    "'''\n",
    "mlm = MaskLM(vocab_size, num_hiddens)\n",
    "'''\n",
    "- **定义预测位置**：\n",
    "    `mlm_positions`表示每个输入序列中需要预测的词元位置。\n",
    "'''\n",
    "mlm_positions = torch.tensor(\n",
    "    [\n",
    "        [1, 5, 2],\n",
    "        [6, 1, 5]\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "- **前向推断**：通过调用`mlm`实例的`forward`方法，计算预测结果`mlm_Y_hat`。\n",
    "    输出形状为（批量大小，预测位置数量，词表大小）。\n",
    "'''\n",
    "mlm_Y_hat = mlm(encoded_X, mlm_positions)\n",
    "mlm_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2709ebae-4799-444a-8994-f5b304207012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "计算损失\n",
    "\n",
    "- **定义真实标签**：`mlm_Y`是掩蔽位置的真实标签。\n",
    "'''\n",
    "mlm_Y = torch.tensor(\n",
    "    [\n",
    "        [7, 8, 9],\n",
    "        [10, 20, 30]\n",
    "    ]\n",
    ")\n",
    "'''\n",
    "- **损失函数**：\n",
    "    使用交叉熵损失函数计算预测结果和真实标签之间的差距。\n",
    "  - `reduction='none'`：不进行损失的平均或求和操作。\n",
    "'''\n",
    "loss = nn.CrossEntropyLoss(reduction='none')\n",
    "'''\n",
    "- **计算损失**：\n",
    "  - `mlm_Y_hat.reshape((-1, vocab_size))`：\n",
    "      将预测结果调整为二维张量，形状为（批量大小 * 预测位置数量，词表大小）。\n",
    "  - `mlm_Y.reshape(-1)`：\n",
    "      将真实标签调整为一维张量，形状为（批量大小 * 预测位置数量）。\n",
    "  - `mlm_l = loss(...)`：\n",
    "      计算损失，输出形状为（批量大小 * 预测位置数量）。\n",
    "'''\n",
    "mlm_l = loss(\n",
    "    mlm_Y_hat.reshape(\n",
    "        (-1, vocab_size)\n",
    "    ),\n",
    "    mlm_Y.reshape(-1)\n",
    ")\n",
    "'''\n",
    "BERT的掩蔽语言模型任务可以利用双向上下文信息来预测被掩蔽的词元，\n",
    "从而使模型能够更好地理解和生成自然语言。\n",
    "'''\n",
    "mlm_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa34fab-9d6e-4fb6-bf09-60d1ab58ce69",
   "metadata": {},
   "source": [
    "---\n",
    "下一句预测\n",
    "\n",
    "这段代码实现并演示了BERT模型中的下一句预测（Next Sentence Prediction, NSP）任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7e28316-43f3-45f3-a687-c98a3ffe0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextSentencePred(nn.Module):\n",
    "    '''\n",
    "    Bert的下一句预测任务\n",
    "    '''\n",
    "    def __init__(self, num_inputs, **kwargs):\n",
    "        super(NextSentencePred, self).__init__(**kwargs)\n",
    "        self.output = nn.Linear(num_inputs, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        X的形状：（batchsize, num_hiddens）\n",
    "        - `forward` 方法中，输入 `X` 的形状为 `(batch_size, num_hiddens)`。\n",
    "        - 使用线性层 `self.output` 计算输出，\n",
    "            返回形状为 `(batch_size, 2)` 的张量，\n",
    "            每行对应一个输入序列的二分类结果。\n",
    "\n",
    "        '''\n",
    "        return self.output(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5247193a-b638-4c72-900a-8007b4d35bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "准备输入数据\n",
    "- 这里的 `encoded_X` 是由BERT编码器生成的表示，\n",
    "    形状为 `(batch_size, seq_length, num_hiddens)`。\n",
    "- 使用 `torch.flatten` 方法，\n",
    "    将其展平为 `(batch_size, num_hiddens)` 的形状，以适应NSP的输入要求。\n",
    "'''\n",
    "encoded_X = torch.flatten(encoded_X, start_dim=1)\n",
    "'''\n",
    "创建NSP实例并进行前向推断\n",
    "- 创建一个 `NextSentencePred` 的实例 `nsp`，\n",
    "    其输入大小为 `encoded_X` 的最后一个维度大小（即 `num_hiddens`）。\n",
    "- 对 `encoded_X` 进行前向推断，\n",
    "    得到 `nsp_Y_hat`，其形状为 `(batch_size, 2)`。\n",
    "'''\n",
    "nsp = NextSentencePred(encoded_X.shape[-1])\n",
    "nsp_Y_hat = nsp(encoded_X)\n",
    "nsp_Y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bed63295-3618-4149-aa72-1999c6f312ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "准备标签并计算损失\n",
    "\n",
    "- `nsp_y` 是真实的标签，表示两个输入序列的下一句预测结果。\n",
    "    `0` 表示下一句是连续的，`1` 表示下一句不是连续的。\n",
    "- 使用交叉熵损失函数 `loss` \n",
    "    计算 `nsp_Y_hat` 和 `nsp_y` 之间的损失 `nsp_l`。\n",
    "'''\n",
    "nsp_y = torch.tensor([0, 1])\n",
    "nsp_l = loss(nsp_Y_hat, nsp_y)\n",
    "nsp_l.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60651ee1-58b3-4bad-a38e-a72e4c784a59",
   "metadata": {},
   "source": [
    "1. **定义NSP类**：实现一个简单的多层感知机（MLP）来进行二分类。\n",
    "2. **准备输入数据**：将BERT编码器生成的表示展平，以适应NSP的输入要求。\n",
    "3. **创建NSP实例并进行前向推断**：生成二分类结果。\n",
    "4. **准备标签并计算损失**：使用真实标签计算预测结果的损失。\n",
    "\n",
    "通过这段代码，我们实现并演示了如何使用BERT模型进行下一句预测任务。这一任务有助于模型理解句子之间的逻辑关系，从而提升其在自然语言处理任务中的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315cef0e-9396-4f9d-a0b0-24d01367eaad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
