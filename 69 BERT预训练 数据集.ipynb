{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57e0155-b2e5-439c-a683-d92ad1a70dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f12302-f9b7-4fcb-a551-a8f2174d1bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n数据集下载：\\nhttps://github.com/Snail1502/dataset_d2l/blob/main/wikitext-2-v1.zip\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "数据集下载：\n",
    "https://github.com/Snail1502/dataset_d2l/blob/main/wikitext-2-v1.zip\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37063fad-e173-49ef-9f8a-f09bd193aa1c",
   "metadata": {},
   "source": [
    "从WikiText-2数据集中读取训练数据，并对其进行预处理，以便于BERT模型的预训练。\n",
    "\n",
    "这段代码的核心功能是从WikiText-2数据集中读取训练数据，并将段落按句号分割成句子，同时确保每个段落至少包含两个句子。通过将所有字母转换为小写和打乱段落顺序，进一步规范化数据并增强模型的泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23037444-9f61-425b-b6ce-bf6c23c39c60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    '''\n",
    "    大写字母转小写字母\n",
    "    \n",
    "    **处理段落**\n",
    "    \n",
    "    - `line.strip().lower()`：\n",
    "        去除行首行尾的空白符，并将所有字母转换为小写。\n",
    "    - `line.split(' . ')`：\n",
    "        将段落按句号（`.`）分隔成多个句子。\n",
    "    - `if len(line.split(' . ')) >= 2`：\n",
    "        只保留包含至少两个句子的段落。\n",
    "    '''\n",
    "    paragraphs = [\n",
    "        line.strip().lower().split(' . ')\n",
    "        for line in lines if len(line.split(' . ')) >= 2\n",
    "    ]\n",
    "    '''\n",
    "    **打乱段落顺序**\n",
    "    \n",
    "    使用 `random.shuffle` 将段落的顺序随机打乱，以增强模型的泛化能力。\n",
    "    '''\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa93ffa-a621-4953-b5f1-96e06781bb5b",
   "metadata": {},
   "source": [
    "为预训练任务定义辅助函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a293a992-8baf-495b-b5b2-127212ca7931",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0e240fb-e2b2-4194-8971-d283052819c4",
   "metadata": {},
   "source": [
    "`_get_next_sentence` 和 `_get_nsp_data_from_paragraph`的目的是:\n",
    "生成用于BERT模型预训练的下一句预测任务的数据。\n",
    "\n",
    "在BERT的预训练任务中，模型需要判断两个句子是否连续出现，这被称为“下一句预测（NSP）”任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ba0584-551f-46ad-bd6f-fc7cf35e46d7",
   "metadata": {},
   "source": [
    "生成下一句预测的数据\n",
    "\n",
    "这个函数生成一个二分类任务的训练样本，即判断两个句子是否是连续的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4467b25e-6189-4c36-a493-0a8435a67e56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_next_sentence(sentence, next_sentence, paragraphs):\n",
    "    '''\n",
    "    **随机选择判断标准**\n",
    "    \n",
    "    使用 `random.random()` 生成一个 0 到 1 之间的随机数。\n",
    "    如果这个数小于 0.5，则 `is_next` 设为 `True`，\n",
    "    表示 `next_sentence` 是当前 `sentence` 的下一句。\n",
    "    \n",
    "    否则，从段落列表 `paragraphs` 中随机选择一个句子作为 `next_sentence`，\n",
    "    并将 `is_next` 设为 `False`，\n",
    "    表示 `next_sentence` 不是当前 `sentence` 的下一句。\n",
    "\n",
    "    '''\n",
    "    if random.random() < 0.5:\n",
    "        is_next = True\n",
    "    else:\n",
    "        '''\n",
    "        paragraphs是三重列表的嵌套\n",
    "        '''\n",
    "        next_sentence = random.choice(\n",
    "            random.choice(paragraphs)\n",
    "        )\n",
    "        is_next = False\n",
    "    '''\n",
    "    返回当前句子 `sentence`，\n",
    "    下一句 `next_sentence`，\n",
    "    以及布尔值 `is_next`，表示 `next_sentence` 是否是 `sentence` 的下一句。\n",
    "    '''\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2e80500-0a65-4fb9-9290-810aef8064aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tokens_and_segments(tokens_a, tokens_b=None):\n",
    "    \"\"\"获取输入序列的词元及其片段索引\n",
    "\n",
    "    Defined in :numref:`sec_bert`\"\"\"\n",
    "    tokens = ['<cls>'] + tokens_a + ['<sep>']\n",
    "    # 0和1分别标记片段A和B\n",
    "    segments = [0] * (len(tokens_a) + 2)\n",
    "    if tokens_b is not None:\n",
    "        tokens += tokens_b + ['<sep>']\n",
    "        segments += [1] * (len(tokens_b) + 1)\n",
    "    return tokens, segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66569a2-5f4b-4bd3-8e72-236ec0db6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "这个函数通过调用 `get_next_sentence` 函数从输入的段落生成用于下一句预测的训练样本。\n",
    "\n",
    "'''\n",
    "def get_nsp_data_from_paragraph(\n",
    "    paragraph, paragraphs, vocab, max_len\n",
    "):\n",
    "    '''\n",
    "    **初始化数据列表**\n",
    "    nsp_data_from_paragraph 用于存储生成的训练样本。\n",
    "    '''\n",
    "    nsp_data_from_paragraph = []\n",
    "    '''\n",
    "    **遍历段落中的句子**\n",
    "    遍历段落中的句子，对每一对相邻句子调用 `get_next_sentence` 函数生成训练样本。\n",
    "    `paragraph[i]` 和 `paragraph[i + 1]` 是相邻的两个句子。\n",
    "    '''\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        tokens_a, tokens_b, is_next = get_next_sentence(\n",
    "            paragraph[i], paragraph[i+1], paragraphs\n",
    "        )\n",
    "        '''\n",
    "        **检查长度约束**\n",
    "        \n",
    "        考虑1个'<cls>'词元和2个'<sep>'词元\n",
    "        \n",
    "        检查生成的句子对的长度是否超过 `max_len`（考虑到一个 `<cls>` 词元和两个 `<sep>` 词元）。\n",
    "        如果超过，则跳过这个样本。\n",
    "        '''\n",
    "        if len(tokens_a) + len(tokens_b) + 3 > max_len:\n",
    "            continue\n",
    "        '''\n",
    "        **获取词元和片段**\n",
    "        使用 `get_tokens_and_segments` 函数获取词元和片段标记。\n",
    "        '''\n",
    "        # tokens, segments = d2l.get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        tokens, segments = get_tokens_and_segments(tokens_a, tokens_b)\n",
    "        \n",
    "        '''\n",
    "        **添加到数据列表**\n",
    "        \n",
    "        将生成的词元、片段标记和 `is_next` 标志\n",
    "        添加到 `nsp_data_from_paragraph` 列表中。\n",
    "        '''\n",
    "        nsp_data_from_paragraph.append(\n",
    "            (tokens, segments, is_next)\n",
    "        )\n",
    "        \n",
    "    return nsp_data_from_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03afc4b4-8370-493d-9e7a-c76fc274ff8f",
   "metadata": {},
   "source": [
    "总结\n",
    "\n",
    "这段代码的两个辅助函数 `_get_next_sentence` 和 `_get_nsp_data_from_paragraph` 分别实现了生成下一句预测任务的单个样本和从段落生成多个训练样本的功能。通过这些函数，可以将原始的文本语料库转换为适合BERT预训练的数据格式。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b0197-0ea5-43ee-b25c-2f2b27f73000",
   "metadata": {},
   "source": [
    "`replace_mlm_tokens`,`get_mlm_data_from_tokens`'的目的是:\n",
    "生成用于BERT模型预训练的遮蔽语言模型（MLM）任务的数据。\n",
    "MLM任务通过随机遮蔽输入序列中的一些词元，并让模型预测这些被遮蔽的词元，从而使模型能够更好地理解上下文。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ef96a6-a2d3-4fca-977e-6bcbbcad95ec",
   "metadata": {},
   "source": [
    "生成遮蔽语言模型任务的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9441f864-e80b-4565-9353-5747835f8383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "这个函数负责将输入序列中的一部分词元替换为 `<mask>` 或其他随机词元，\n",
    "并记录这些替换的位置和原始词元。\n",
    "\n",
    "'''\n",
    "def replace_mlm_tokens(\n",
    "    tokens, \n",
    "    candidate_pred_positions, \n",
    "    num_mlm_preds,\n",
    "    vocab\n",
    "):\n",
    "    '''\n",
    "    为遮蔽语言模型的输入创建新的词元副本，\n",
    "    其中输入可能包含替换的“<mask>”或随机词元\n",
    "    \n",
    "    创建一个新的词元列表 `mlm_input_tokens`，\n",
    "    它是输入 `tokens` 的副本，用于进行遮蔽和替换操作。\n",
    "    '''\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    \n",
    "    '''\n",
    "    初始化一个空列表 `pred_positions_and_labels`，\n",
    "    用于存储被替换的词元位置及其原始词元。\n",
    "    '''\n",
    "    pred_positions_and_labels = []\n",
    "    \n",
    "    '''\n",
    "    打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
    "    \n",
    "    将候选的预测位置 `candidate_pred_positions` 随机打乱，\n",
    "    以便从中随机选择15%的词元进行预测。\n",
    "    '''\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    \n",
    "    '''\n",
    "    **替换词元**\n",
    "    \n",
    "    对于每一个候选的预测位置 `mlm_pred_position`，根据以下概率进行替换：\n",
    "    - 80%的概率将词元替换为 `<mask>`。\n",
    "    - 10%的概率保持词元不变。\n",
    "    - 10%的概率将词元替换为词汇表中的随机词元。\n",
    "\n",
    "    每次替换后，将位置和原始词元添加到 `pred_positions_and_labels` 中，\n",
    "    并更新 `mlm_input_tokens`。\n",
    "\n",
    "    '''\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        '''\n",
    "        80%的时间：将词替换为“<mask>”词元\n",
    "        '''\n",
    "        masked_token = None\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '<mask>'\n",
    "        else:\n",
    "            '''\n",
    "            10%的时间：保持词不变\n",
    "            '''\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = tokens[mlm_pred_position]\n",
    "            else:\n",
    "                '''\n",
    "                10%的时间：用随机词替换该词\n",
    "                '''\n",
    "                masked_token = random.choice(vocab.idx_to_token)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append(\n",
    "            (\n",
    "                mlm_pred_position, tokens[mlm_pred_position]\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        返回可能替换后的输入词元列表 `mlm_input_tokens`，\n",
    "        以及发生预测的词元位置和原始词元的列表 `pred_positions_and_labels`\n",
    "        '''\n",
    "        return mlm_input_tokens, pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e8a01d9-8195-499d-bd0c-719426885508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "这个函数通过调用 `_replace_mlm_tokens` 函数，生成用于遮蔽语言模型任务的数据。\n",
    "'''\n",
    "def get_mlm_data_from_tokens(tokens, vocab):\n",
    "    '''\n",
    "    **初始化候选位置列表**\n",
    "    \n",
    "    遍历输入的词元列表 `tokens`，\n",
    "    将所有非特殊词元的位置添加到 `candidate_pred_positions` 列表中。\n",
    "    \n",
    "    特殊词元（如 `<cls>` 和 `<sep>`）不会在遮蔽语言模型任务中被预测。\n",
    "    '''\n",
    "    candidate_pred_positions = []\n",
    "    '''\n",
    "    tokens是一个字符串列表\n",
    "    '''\n",
    "    for i, token in enumerate(tokens):\n",
    "        '''\n",
    "        在遮蔽语言模型任务中不会预测特殊词元\n",
    "        '''\n",
    "        if token in ['<cls>', '<sep>']:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "    '''\n",
    "    **计算要预测的词元数量**\n",
    "    计算需要预测的词元数量，约为总词元数量的15%。确保至少预测一个词元。\n",
    "    \n",
    "    遮蔽语言模型任务中预测15%的随机词元\n",
    "    '''\n",
    "    num_mlm_preds = max(\n",
    "        1, round(len(tokens) * 0.15)\n",
    "    )\n",
    "    '''\n",
    "    **调用替换函数**\n",
    "    调用 `_replace_mlm_tokens` 函数，生成遮蔽后的输入词元和预测位置及其原始词元。\n",
    "    '''\n",
    "    mlm_input_tokens, pred_positions_and_labels = replace_mlm_tokens(\n",
    "        tokens, candidate_pred_positions, num_mlm_preds, vocab\n",
    "    )\n",
    "    '''\n",
    "    **排序预测位置和标签**\n",
    "    \n",
    "    对预测位置和标签进行排序，\n",
    "    并分别提取预测位置 `pred_positions` 和预测标签 `mlm_pred_labels`。\n",
    "    '''\n",
    "    pred_positions_and_labels = sorted(\n",
    "        pred_positions_and_labels, key=lambda x: x[0]\n",
    "    )\n",
    "    \n",
    "    pred_positions = [\n",
    "        v[0] for v in pred_positions_and_labels\n",
    "    ]\n",
    "    mlm_pred_labels = [\n",
    "        v[1] for v in pred_positions_and_labels\n",
    "    ]\n",
    "    '''\n",
    "    返回词汇表中对应的输入词元索引 `vocab[mlm_input_tokens]`，\n",
    "    预测位置 `pred_positions`，以及预测标签的索引 `vocab[mlm_pred_labels]`。\n",
    "    '''\n",
    "    return vocab[mlm_input_tokens], pred_positions, vocab[mlm_pred_labels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f2b84-ec84-4886-8c3a-b3c80ff1f633",
   "metadata": {},
   "source": [
    "总结\n",
    "\n",
    "这段代码定义了两个函数 `_replace_mlm_tokens` 和 `_get_mlm_data_from_tokens`，用于生成BERT预训练任务中的遮蔽语言模型数据。前者负责将部分词元替换为 `<mask>` 或随机词元，并记录替换位置和原始词元；后者负责调用前者，生成遮蔽后的输入词元及其对应的预测位置和标签。通过这些函数，可以将原始的文本序列转换为适合BERT预训练的数据格式。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b951511-a550-4f66-b9e6-8063264657a4",
   "metadata": {},
   "source": [
    "将文本转化为预训练数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbde364-fd85-4699-b29e-9fc021eb3174",
   "metadata": {},
   "source": [
    "pad_bert_inputs, WikiTextDataset, load_data_wiki的目的是:\n",
    "    将文本数据转换为适合BERT模型预训练的数据集，包括下一句预测（NSP）和遮蔽语言模型（MLM）任务。代码实现了数据的预处理、词元化、填充以及数据加载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe288136-a735-4617-9b42-963fa7c057bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "这个函数用于将BERT的输入填充到统一的长度，并准备好所有训练所需的数据。\n",
    "'''\n",
    "\n",
    "def pad_bert_inputs(\n",
    "    examples, max_len, vocab\n",
    "):\n",
    "    '''\n",
    "    初始化多个列表，用于存储填充后的各项数据。\n",
    "    `max_num_mlm_preds` 表示遮蔽语言模型任务中最多的预测词元数（即输入序列长度的15%）。\n",
    "    '''\n",
    "    max_num_mlm_preds = round(max_len * 0.15)\n",
    "    all_token_ids, all_segments, valid_lens = [], [], []\n",
    "    all_pred_positions, all_mlm_weights, all_mlm_labels = [], [], []\n",
    "    nsp_labels = [] \n",
    "    '''\n",
    "    **遍历每个示例**：\n",
    "    遍历输入的每个示例，每个示例包含了BERT输入序列的各项信息。\n",
    "    '''\n",
    "    for (\n",
    "        token_ids, pred_positions, mlm_pred_label_ids, segments, is_next\n",
    "    ) in examples:\n",
    "        '''\n",
    "        **填充和处理各项数据**：\n",
    "\n",
    "        - `token_ids` 填充到 `max_len` 长度，填充的词元为 `<pad>`。\n",
    "        '''\n",
    "        all_token_ids.append(\n",
    "            torch.tensor(\n",
    "                token_ids + [vocab['<pad>']] * (max_len - len(token_ids)),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        - `segments` 填充到 `max_len` 长度，填充的值为 0。\n",
    "        '''\n",
    "        all_segments.append(\n",
    "            torch.tensor(\n",
    "                segments + [0] * (max_len - len(token_ids)),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        - `valid_lens` 表示真实的词元长度，不包括填充部分。\n",
    "        '''\n",
    "        valid_lens.append(\n",
    "            torch.tensor(\n",
    "                len(token_ids),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        - `pred_positions` 填充到 `max_num_mlm_preds` 长度，填充的值为 0。\n",
    "        '''\n",
    "        all_pred_positions.append(\n",
    "            torch.tensor(\n",
    "                pred_positions + [0]*(max_num_mlm_preds - len(pred_positions)),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        - `mlm_weights` 用于遮蔽语言模型的损失计算，填充值为0的部分不会在损失中计算。\n",
    "        '''\n",
    "        all_mlm_weights.append(\n",
    "            torch.tensor(\n",
    "                [1.0] * len(mlm_pred_label_ids) + [0.0] * (max_num_mlm_preds - len(pred_positions)),\n",
    "                dtype=torch.float32\n",
    "            )\n",
    "            \n",
    "        )\n",
    "        '''\n",
    "        - `mlm_labels` 填充到 `max_num_mlm_preds` 长度，填充的值为 0。\n",
    "        '''\n",
    "        all_mlm_labels.append(\n",
    "            torch.tensor(\n",
    "                mlm_pred_label_ids + [0] * (max_num_mlm_preds - len(mlm_pred_label_ids )),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "        '''\n",
    "        - `nsp_labels` 存储下一句预测的标签。\n",
    "        '''\n",
    "        nsp_labels.append(\n",
    "            torch.tensor(\n",
    "                is_next, dtype=torch.long\n",
    "            )\n",
    "        )\n",
    "    '''\n",
    "    返回填充后的各项数据。\n",
    "    '''\n",
    "    return (\n",
    "        all_token_ids,\n",
    "        all_segments,\n",
    "        valid_lens,\n",
    "        all_pred_positions,\n",
    "        all_mlm_weights,\n",
    "        all_mlm_labels,\n",
    "        nsp_labels\n",
    "    )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "161f4500-d19b-49ab-8122-38e430edd7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "这个类用于将WikiText-2数据集转换为适合BERT预训练的数据格式。\n",
    "'''\n",
    "class WikiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paragraphs, max_len):\n",
    "        '''\n",
    "        - 将段落 `paragraphs` 进行词元化，结果是一个列表的列表，每个列表是一个段落的词元列表。\n",
    "        '''\n",
    "        paragraphs = [\n",
    "            d2l.tokenize(\n",
    "                paragraph,\n",
    "                token='word'\n",
    "            ) for paragraph in paragraphs\n",
    "        ]\n",
    "        '''\n",
    "        - 将所有段落中的句子提取出来，形成 `sentences` 列表。\n",
    "        '''\n",
    "        sentences = [\n",
    "            sentence for paragraph in paragraphs for sentence in paragraph\n",
    "        ]\n",
    "        '''\n",
    "        - 创建词汇表 `vocab`，过滤掉出现次数少于5次的词元，并预留特殊词元。\n",
    "        '''\n",
    "        self.vocab = d2l.Vocab(\n",
    "            sentences, \n",
    "            min_freq=5, \n",
    "            reserved_tokens=[\n",
    "                '<pad>', '<mask>', '<cls>', '<sep>'\n",
    "            ]\n",
    "        )\n",
    "        '''\n",
    "        **生成NSP任务的数据**\n",
    "        调用 `_get_nsp_data_from_paragraph` 函数，\n",
    "        为每个段落生成下一句预测任务的数据。\n",
    "        '''\n",
    "        examples = []\n",
    "        for paragraph in paragraphs:\n",
    "            examples.extend(\n",
    "                get_nsp_data_from_paragraph(\n",
    "                    paragraph, paragraphs, self.vocab, max_len\n",
    "                )\n",
    "            )\n",
    "        '''\n",
    "        **生成MLM任务的数据**：\n",
    "        调用 `get_mlm_data_from_tokens` 函数，\n",
    "        为每个示例生成遮蔽语言模型任务的数据。\n",
    "        '''\n",
    "        examples = [\n",
    "            (\n",
    "                get_mlm_data_from_tokens(\n",
    "                    tokens, self.vocab\n",
    "                ) + (\n",
    "                    segments, is_next\n",
    "                )\n",
    "            ) for tokens, segments, is_next in examples\n",
    "        ]\n",
    "        '''\n",
    "        **填充数据**：\n",
    "        调用 `_pad_bert_inputs` 函数，将所有示例的数据进行填充。  \n",
    "        '''\n",
    "        (\n",
    "            self.all_token_ids,\n",
    "            self.all_segments,\n",
    "            self.valid_lens,\n",
    "            self.all_pred_positions,\n",
    "            self.all_mlm_weights,\n",
    "            self.all_mlm_labels,\n",
    "            self.nsp_labels\n",
    "        ) = pad_bert_inputs(\n",
    "            examples, max_len, self.vocab\n",
    "        )\n",
    "    '''\n",
    "    - `__getitem__` 方法用于获取指定索引 `idx` 的示例数据。\n",
    "    '''\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.all_token_ids[idx],\n",
    "            self.all_segments[idx],\n",
    "            self.valid_lens[idx],\n",
    "            self.all_pred_positions[idx],\n",
    "            self.all_mlm_weights[idx],\n",
    "            self.all_mlm_labels[idx],\n",
    "            self.nsp_labels[idx]\n",
    "        )\n",
    "    '''\n",
    "    - `__len__` 方法返回数据集的大小。\n",
    "    '''\n",
    "    def __len__(self):\n",
    "        return len(self.all_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f905c8b3-36b2-42f3-891f-9c63ae391966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "这个函数用于加载WikiText-2数据集并生成用于预训练的样本。\n",
    "'''\n",
    "def load_data_wiki(batch_size, max_len):\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    data_dir = './data/wikitext-2/'\n",
    "    paragraphs = read_wiki(data_dir)\n",
    "    train_set = WikiTextDataset(paragraphs, max_len)\n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        train_set, \n",
    "        batch_size, \n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_iter, train_set.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f47fd9c-5ee6-4bad-8f0c-757f240a17d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 64]) torch.Size([512, 64]) torch.Size([512]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512, 10]) torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "batch_size, max_len = 512, 64\n",
    "train_iter, vocab = load_data_wiki(batch_size, max_len)\n",
    "\n",
    "for (\n",
    "    tokens_X, \n",
    "    segments_X,\n",
    "    valid_lens_X,\n",
    "    pred_positions_X,\n",
    "    mlm_weights_X,\n",
    "    mlm_Y,\n",
    "    nsp_Y\n",
    ") in train_iter:\n",
    "    print(\n",
    "        tokens_X.shape,\n",
    "        segments_X.shape,\n",
    "        valid_lens_X.shape,\n",
    "        pred_positions_X.shape,\n",
    "        mlm_weights_X.shape,\n",
    "        mlm_Y.shape,\n",
    "        nsp_Y.shape\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3d38a3e-5615-48ad-ad83-55b864a3a822",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
